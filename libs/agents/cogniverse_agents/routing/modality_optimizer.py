"""
Modality Optimizer

Per-modality routing optimization using XGBoost meta-learning for automatic decisions.
Part of Phase 11: Multi-Modal Optimization.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import dspy
from dspy.teleprompt import BootstrapFewShot, MIPROv2

from cogniverse_agents.routing.modality_evaluator import ModalityEvaluator
from cogniverse_agents.routing.modality_example import ModalityExample
from cogniverse_agents.routing.modality_span_collector import ModalitySpanCollector
from cogniverse_agents.routing.xgboost_meta_models import (
    ModelingContext,
    TrainingDecisionModel,
    TrainingStrategy,
    TrainingStrategyModel,
)
from cogniverse_agents.search.multi_modal_reranker import QueryModality
from cogniverse_synthetic import (
    ModalityExampleSchema,
    SyntheticDataRequest,
    SyntheticDataService,
)

logger = logging.getLogger(__name__)


class ModalityRoutingSignature(dspy.Signature):
    """Modality-specific routing decision"""

    query = dspy.InputField(desc="User query")
    modality = dspy.InputField(
        desc="Query modality (video, image, audio, document, text)"
    )
    query_features = dspy.InputField(desc="Extracted query features as JSON")

    recommended_agent = dspy.OutputField(
        desc="Recommended agent (video_search, image_search, audio_analysis, document_agent, text_search)"
    )
    confidence = dspy.OutputField(desc="Confidence in recommendation (0-1)")
    reasoning = dspy.OutputField(desc="Reasoning for this routing choice")


class ModalityRoutingModule(dspy.Module):
    """DSPy module for modality-specific routing"""

    def __init__(self):
        super().__init__()
        self.route = dspy.ChainOfThought(ModalityRoutingSignature)

    def forward(self, query, modality, query_features=None):
        features_str = json.dumps(query_features or {}, default=str)

        result = self.route(
            query=query,
            modality=(
                modality.value if isinstance(modality, QueryModality) else modality
            ),
            query_features=features_str,
        )

        return result


class ModalityOptimizer:
    """
    Per-modality routing optimizer with XGBoost meta-learning

    Key Features:
    - Separate optimization per modality (VIDEO, DOCUMENT, IMAGE, AUDIO)
    - XGBoost meta-models for automatic decision-making
    - Synthetic data generation for cold start
    - Progressive training strategies (synthetic â†’ hybrid â†’ pure real)
    - Tracks training history and performance metrics

    Usage:
        optimizer = ModalityOptimizer(tenant_id="default")

        # Evaluate and potentially train all modalities
        results = await optimizer.optimize_all_modalities()

        # Or optimize specific modality
        result = await optimizer.optimize_modality(QueryModality.VIDEO)
    """

    def __init__(
        self,
        tenant_id: str = "default",
        model_dir: Optional[Path] = None,
        vespa_client=None,
        backend_config: Optional[Dict[str, Any]] = None,
    ):
        """
        Initialize modality optimizer

        Args:
            tenant_id: Tenant identifier for multi-tenancy
            model_dir: Directory for saving models (defaults to outputs/models/modality)
            vespa_client: Optional Vespa client for synthetic data generation
            backend_config: Optional backend configuration for synthetic data generation
        """
        self.tenant_id = tenant_id
        self.model_dir = model_dir or Path("outputs/models/modality")
        self.model_dir.mkdir(parents=True, exist_ok=True)

        # Initialize components
        self.span_collector = ModalitySpanCollector(tenant_id)
        self.evaluator = ModalityEvaluator(self.span_collector, tenant_id)
        self.vespa_client = vespa_client  # Store for synthetic data generation
        self.backend_config = backend_config  # Store for synthetic data generation

        # Initialize XGBoost meta-models
        self.training_decision_model = TrainingDecisionModel(model_dir=self.model_dir)
        self.training_strategy_model = TrainingStrategyModel(model_dir=self.model_dir)

        # Load existing meta-models if available
        self.training_decision_model.load()
        self.training_strategy_model.load()

        # Training history per modality
        self.training_history: Dict[QueryModality, List[Dict[str, Any]]] = {}

        # Trained DSPy models per modality
        self.modality_models: Dict[QueryModality, ModalityRoutingModule] = {}

        # Load existing trained models if available
        self._load_trained_models()

        logger.info(
            f"ðŸ”§ Initialized ModalityOptimizer for tenant '{tenant_id}' "
            f"(model_dir: {self.model_dir})"
        )

    async def optimize_all_modalities(
        self,
        lookback_hours: int = 24,
        min_confidence: float = 0.7,
    ) -> Dict[QueryModality, Dict[str, Any]]:
        """
        Evaluate and optimize all modalities

        Args:
            lookback_hours: How far back to look for spans
            min_confidence: Minimum confidence threshold for span collection

        Returns:
            Dictionary mapping each modality to optimization results
        """
        logger.info(
            f"ðŸš€ Starting optimization for all modalities "
            f"(lookback: {lookback_hours}h, min_confidence: {min_confidence})"
        )

        results = {}

        # Get span statistics to see which modalities have data
        stats = await self.span_collector.get_modality_statistics(lookback_hours)
        available_modalities = stats.get("modality_distribution", {})

        if not available_modalities:
            logger.warning("âš ï¸ No modality data available - skipping optimization")
            return {}

        # Optimize each modality that has data
        for modality_str in available_modalities:
            try:
                modality = QueryModality(modality_str)
                result = await self.optimize_modality(
                    modality,
                    lookback_hours=lookback_hours,
                    min_confidence=min_confidence,
                )
                results[modality] = result
            except Exception as e:
                logger.error(f"âŒ Error optimizing {modality_str}: {e}")
                results[QueryModality(modality_str)] = {
                    "status": "error",
                    "error": str(e),
                }

        # Log summary
        trained_count = sum(1 for r in results.values() if r.get("trained", False))
        logger.info(
            f"âœ… Optimization complete: {trained_count}/{len(results)} modalities trained"
        )

        return results

    async def optimize_modality(
        self,
        modality: QueryModality,
        lookback_hours: int = 24,
        min_confidence: float = 0.7,
        force_training: bool = False,
    ) -> Dict[str, Any]:
        """
        Evaluate and potentially train a specific modality

        Args:
            modality: Modality to optimize
            lookback_hours: How far back to look for spans
            min_confidence: Minimum confidence threshold
            force_training: Force training regardless of decision model

        Returns:
            Optimization results including decision, strategy, and metrics
        """
        logger.info(f"ðŸŽ¯ Optimizing {modality.value} modality")

        # Step 1: Collect training examples
        training_examples = await self.evaluator.create_training_examples(
            lookback_hours=lookback_hours,
            min_confidence=min_confidence,
            augment_with_synthetic=False,  # We'll handle synthetic separately
        )

        modality_examples = training_examples.get(modality, [])

        logger.info(
            f"ðŸ“Š Collected {len(modality_examples)} real examples for {modality.value}"
        )

        # Step 2: Build modeling context
        context = self._build_modeling_context(modality, modality_examples)

        # Step 3: Decide whether to train
        if not force_training:
            should_train, expected_improvement = (
                self.training_decision_model.should_train(context)
            )

            if not should_train:
                logger.info(
                    f"â­ï¸ Skipping {modality.value} training "
                    f"(expected improvement: {expected_improvement:.3f})"
                )
                return {
                    "modality": modality.value,
                    "trained": False,
                    "reason": "insufficient_benefit",
                    "expected_improvement": expected_improvement,
                    "context": self._context_to_dict(context),
                }
        else:
            expected_improvement = 0.0
            logger.info(f"ðŸ”¨ Force training {modality.value}")

        # Step 4: Select training strategy
        strategy = self.training_strategy_model.select_strategy(context)

        logger.info(f"ðŸ“‹ Selected strategy for {modality.value}: {strategy.value}")

        # Step 5: Prepare training data based on strategy
        training_data = await self._prepare_training_data(
            modality, modality_examples, strategy
        )

        if not training_data:
            logger.warning(f"âš ï¸ No training data available for {modality.value}")
            return {
                "modality": modality.value,
                "trained": False,
                "reason": "no_training_data",
                "strategy": strategy.value,
                "context": self._context_to_dict(context),
            }

        logger.info(
            f"ðŸ“š Prepared {len(training_data)} training examples "
            f"(strategy: {strategy.value})"
        )

        # Step 6: Train modality-specific model (placeholder for now)
        training_result = self._train_modality_model(modality, training_data, strategy)

        # Step 7: Record training history
        self._record_training(modality, context, strategy, training_result)

        return {
            "modality": modality.value,
            "trained": True,
            "strategy": strategy.value,
            "examples_count": len(training_data),
            "expected_improvement": expected_improvement,
            "training_result": training_result,
            "context": self._context_to_dict(context),
        }

    def _build_modeling_context(
        self, modality: QueryModality, examples: List[ModalityExample]
    ) -> ModelingContext:
        """
        Build ModelingContext from current state

        Args:
            modality: Query modality
            examples: Real training examples

        Returns:
            ModelingContext for decision-making
        """
        # Count real vs synthetic examples
        real_examples = [ex for ex in examples if not ex.is_synthetic]
        synthetic_examples = [ex for ex in examples if ex.is_synthetic]

        # Calculate metrics
        success_count = sum(1 for ex in real_examples if ex.success)
        success_rate = success_count / len(real_examples) if real_examples else 0.0

        # Calculate average confidence
        confidences = [
            ex.modality_features.get("routing_confidence", 0.0)
            for ex in real_examples
            if ex.modality_features
        ]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

        # Get days since last training
        days_since_last = self._get_days_since_last_training(modality)

        # Estimate current performance (based on success rate and confidence)
        current_performance = (success_rate * 0.7) + (avg_confidence * 0.3)

        # Estimate data quality (based on feature diversity and confidence variance)
        data_quality = self._estimate_data_quality(real_examples)

        # Estimate feature diversity
        feature_diversity = self._estimate_feature_diversity(real_examples)

        return ModelingContext(
            modality=modality,
            real_sample_count=len(real_examples),
            synthetic_sample_count=len(synthetic_examples),
            success_rate=success_rate,
            avg_confidence=avg_confidence,
            days_since_last_training=days_since_last,
            current_performance_score=current_performance,
            data_quality_score=data_quality,
            feature_diversity=feature_diversity,
        )

    def _get_days_since_last_training(self, modality: QueryModality) -> int:
        """Get days since modality was last trained"""
        history = self.training_history.get(modality, [])
        if not history:
            return 999  # Never trained

        last_training = history[-1]
        last_timestamp = last_training.get("timestamp")
        if not last_timestamp:
            return 999

        days_diff = (datetime.now() - last_timestamp).days
        return days_diff

    def _estimate_data_quality(self, examples: List[ModalityExample]) -> float:
        """
        Estimate data quality based on feature completeness and variance

        Args:
            examples: Training examples

        Returns:
            Quality score (0-1)
        """
        if not examples:
            return 0.0

        # Check feature completeness
        complete_features = sum(
            1
            for ex in examples
            if ex.modality_features and len(ex.modality_features) > 5
        )
        completeness_score = complete_features / len(examples)

        # Check confidence variance (more variance = more diverse scenarios)
        confidences = [
            ex.modality_features.get("routing_confidence", 0.0)
            for ex in examples
            if ex.modality_features
        ]

        if len(confidences) > 1:
            variance = sum(
                (c - sum(confidences) / len(confidences)) ** 2 for c in confidences
            ) / len(confidences)
            variance_score = min(variance * 2, 1.0)  # Scale to 0-1
        else:
            variance_score = 0.0

        # Combine scores
        quality = (completeness_score * 0.7) + (variance_score * 0.3)
        return quality

    def _estimate_feature_diversity(self, examples: List[ModalityExample]) -> float:
        """
        Estimate feature diversity based on unique patterns in queries

        Args:
            examples: Training examples

        Returns:
            Diversity score (0-1)
        """
        if not examples:
            return 0.0

        # Count unique query patterns
        unique_queries = len(set(ex.query.lower().strip() for ex in examples))
        query_diversity = min(unique_queries / len(examples), 1.0)

        # Count unique agents
        unique_agents = len(set(ex.correct_agent for ex in examples))
        agent_diversity = min(unique_agents / 4, 1.0)  # Assume max 4 agents

        # Combine
        diversity = (query_diversity * 0.6) + (agent_diversity * 0.4)
        return diversity

    async def _prepare_training_data(
        self,
        modality: QueryModality,
        real_examples: List[ModalityExample],
        strategy: TrainingStrategy,
    ) -> List[ModalityExample]:
        """
        Prepare training data based on selected strategy

        Args:
            modality: Query modality
            real_examples: Real training examples
            strategy: Selected training strategy

        Returns:
            Combined training data
        """
        if strategy == TrainingStrategy.SKIP:
            return []

        if strategy == TrainingStrategy.PURE_REAL:
            return real_examples

        if strategy == TrainingStrategy.SYNTHETIC:
            # Generate synthetic data directly using SyntheticDataService
            synthetic_count = max(50, len(real_examples) * 2)
            service = SyntheticDataService(
                backend=self.vespa_client, backend_config=self.backend_config
            )
            request = SyntheticDataRequest(optimizer="modality", count=synthetic_count)
            response = await service.generate(request)
            # Convert to ModalityExample objects
            synthetic_examples = [
                ModalityExample.from_schema(ModalityExampleSchema(**ex))
                for ex in response.data
            ]
            return synthetic_examples

        if strategy == TrainingStrategy.HYBRID:
            # Mix real and synthetic directly using SyntheticDataService
            synthetic_count = len(real_examples)  # 1:1 ratio
            service = SyntheticDataService(
                backend=self.vespa_client, backend_config=self.backend_config
            )
            request = SyntheticDataRequest(optimizer="modality", count=synthetic_count)
            response = await service.generate(request)
            # Convert to ModalityExample objects
            synthetic_examples = [
                ModalityExample.from_schema(ModalityExampleSchema(**ex))
                for ex in response.data
            ]
            return real_examples + synthetic_examples

        return real_examples

    def _train_modality_model(
        self,
        modality: QueryModality,
        training_data: List[ModalityExample],
        strategy: TrainingStrategy,
    ) -> Dict[str, Any]:
        """
        Train modality-specific routing model using DSPy

        Trains a ChainOfThought-based routing module using MIPROv2 or BootstrapFewShot
        based on dataset size. The trained model is saved to disk and stored in memory.

        Args:
            modality: Query modality
            training_data: Training examples
            strategy: Training strategy used

        Returns:
            Training results/metrics
        """
        logger.info(
            f"ðŸŽ“ Training {modality.value} model with {len(training_data)} examples "
            f"(strategy: {strategy.value})"
        )

        try:
            # Configure DSPy with LM if not already configured
            # This is needed for training/compilation
            if not dspy.settings.lm:
                try:
                    # Use Ollama with qwen2.5:7b as default
                    ollama_lm = dspy.LM(
                        model="ollama_chat/qwen2.5:7b",
                        api_base="http://localhost:11434",
                        temperature=0.7,
                    )
                    dspy.configure(lm=ollama_lm)
                    logger.info("Configured DSPy with Ollama qwen2.5:7b model")
                except Exception as e:
                    logger.warning(f"Failed to configure Ollama LM: {e}")
                    # Just return unoptimized module for testing
                    routing_module = ModalityRoutingModule()
                    self.modality_models[modality] = routing_module

                    model_path = (
                        self.model_dir / f"{modality.value}_routing_module.json"
                    )
                    routing_module.save(str(model_path))

                    return {
                        "status": "success",
                        "training_samples": len(training_data),
                        "strategy": strategy.value,
                        "optimizer": "none (no LM available)",
                        "validation_accuracy": 0.0,
                        "model_path": str(model_path),
                        "timestamp": datetime.now().isoformat(),
                    }

            # Convert ModalityExample to DSPy examples
            dspy_examples = []
            for example in training_data:
                features_str = json.dumps(example.modality_features or {}, default=str)

                # Create DSPy Example with inputs and expected outputs
                dspy_example = dspy.Example(
                    query=example.query,
                    modality=example.modality.value,
                    query_features=features_str,
                    recommended_agent=example.correct_agent,
                    confidence=str(0.95 if example.success else 0.5),
                    reasoning=f"Route to {example.correct_agent} for {example.modality.value} queries",
                ).with_inputs("query", "modality", "query_features")

                dspy_examples.append(dspy_example)

            # Create routing module
            routing_module = ModalityRoutingModule()

            # Select optimizer based on dataset size
            if len(dspy_examples) >= 50:
                # Use MIPROv2 for larger datasets (metric-aware optimization)
                logger.info(f"Using MIPROv2 optimizer ({len(dspy_examples)} examples)")
                optimizer = MIPROv2()

                # Compile with MIPROv2
                optimized_module = optimizer.compile(
                    routing_module,
                    trainset=dspy_examples,
                    max_bootstrapped_demos=4,
                    max_labeled_demos=8,
                    num_threads=1,
                )
            else:
                # Use BootstrapFewShot for smaller datasets
                logger.info(
                    f"Using BootstrapFewShot optimizer ({len(dspy_examples)} examples)"
                )
                optimizer = BootstrapFewShot(
                    max_bootstrapped_demos=min(len(dspy_examples), 4),
                )

                # Compile with BootstrapFewShot
                optimized_module = optimizer.compile(
                    routing_module,
                    trainset=dspy_examples,
                )

            # Store in memory
            self.modality_models[modality] = optimized_module

            # Save to disk using DSPy's save mechanism
            model_path = self.model_dir / f"{modality.value}_routing_module.json"
            optimized_module.save(str(model_path))

            logger.info(f"âœ… Saved {modality.value} model to {model_path}")

            # Calculate accuracy on training set (as upper bound estimate)
            correct = 0
            for example in dspy_examples[
                : min(20, len(dspy_examples))
            ]:  # Sample validation
                try:
                    prediction = optimized_module.forward(
                        query=example.query,
                        modality=example.modality,
                        query_features=example.query_features,
                    )
                    if prediction.recommended_agent == example.recommended_agent:
                        correct += 1
                except Exception:
                    pass

            validation_accuracy = (
                correct / min(20, len(dspy_examples)) if dspy_examples else 0.0
            )

            return {
                "status": "success",
                "training_samples": len(training_data),
                "strategy": strategy.value,
                "optimizer": (
                    "MIPROv2" if len(dspy_examples) >= 50 else "BootstrapFewShot"
                ),
                "validation_accuracy": validation_accuracy,
                "model_path": str(model_path),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(
                f"âŒ Failed to train {modality.value} model: {e}", exc_info=True
            )
            return {
                "status": "error",
                "training_samples": len(training_data),
                "strategy": strategy.value,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def _record_training(
        self,
        modality: QueryModality,
        context: ModelingContext,
        strategy: TrainingStrategy,
        result: Dict[str, Any],
    ):
        """Record training in history"""
        if modality not in self.training_history:
            self.training_history[modality] = []

        self.training_history[modality].append(
            {
                "timestamp": datetime.now(),
                "context": self._context_to_dict(context),
                "strategy": strategy.value,
                "result": result,
            }
        )

        logger.info(
            f"ðŸ“ Recorded training for {modality.value} "
            f"(total history: {len(self.training_history[modality])})"
        )

    def _context_to_dict(self, context: ModelingContext) -> Dict[str, Any]:
        """Convert ModelingContext to dictionary"""
        return {
            "modality": context.modality.value,
            "real_sample_count": context.real_sample_count,
            "synthetic_sample_count": context.synthetic_sample_count,
            "success_rate": context.success_rate,
            "avg_confidence": context.avg_confidence,
            "days_since_last_training": context.days_since_last_training,
            "current_performance_score": context.current_performance_score,
            "data_quality_score": context.data_quality_score,
            "feature_diversity": context.feature_diversity,
        }

    def _load_trained_models(self):
        """Load existing trained models from disk"""
        for modality in QueryModality:
            model_path = self.model_dir / f"{modality.value}_routing_module.json"
            if model_path.exists():
                try:
                    # Create new module and load state
                    model = ModalityRoutingModule()
                    model.load(str(model_path))
                    self.modality_models[modality] = model
                    logger.info(f"âœ… Loaded {modality.value} model from {model_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸  Failed to load {modality.value} model: {e}")

    def get_training_history(
        self, modality: Optional[QueryModality] = None
    ) -> Dict[QueryModality, List[Dict[str, Any]]]:
        """
        Get training history

        Args:
            modality: Optional specific modality, or None for all

        Returns:
            Training history
        """
        if modality:
            return {modality: self.training_history.get(modality, [])}
        return self.training_history

    def get_optimization_summary(self) -> Dict[str, Any]:
        """
        Get summary of all modality optimizations

        Returns:
            Summary statistics
        """
        summary = {
            "total_modalities": len(self.training_history),
            "modalities": {},
            "meta_models": {
                "training_decision_model": self.training_decision_model.is_trained,
                "training_strategy_model": self.training_strategy_model.is_trained,
            },
        }

        for modality, history in self.training_history.items():
            if history:
                last_training = history[-1]
                summary["modalities"][modality.value] = {
                    "training_count": len(history),
                    "last_training": last_training["timestamp"].isoformat(),
                    "last_strategy": last_training["strategy"],
                    "last_result": last_training["result"],
                }

        return summary

    def predict_agent(
        self,
        query: str,
        modality: QueryModality,
        query_features: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        Predict the best agent for a query using trained modality model

        Args:
            query: User query text
            modality: Query modality
            query_features: Optional query features

        Returns:
            Prediction result with agent, confidence, reasoning, or None if no model trained
        """
        if modality not in self.modality_models:
            logger.debug(f"No trained model for {modality.value}")
            return None

        try:
            model = self.modality_models[modality]
            result = model.forward(
                query=query,
                modality=modality,
                query_features=query_features,
            )

            return {
                "recommended_agent": result.recommended_agent,
                "confidence": (
                    float(result.confidence)
                    if isinstance(result.confidence, str)
                    else result.confidence
                ),
                "reasoning": result.reasoning,
                "modality": modality.value,
            }
        except Exception as e:
            logger.error(f"Error predicting with {modality.value} model: {e}")
            return None
