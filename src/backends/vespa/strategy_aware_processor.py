"""
Strategy-aware document processor for Vespa ingestion.
This shows how processing should use the extracted ranking strategies.
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Set
import numpy as np

logger = logging.getLogger(__name__)


class StrategyAwareProcessor:
    """Process documents based on ranking strategy requirements"""
    
    def __init__(self):
        self.ranking_strategies = self._load_ranking_strategies()
    
    def _load_ranking_strategies(self) -> Dict[str, Dict[str, Any]]:
        """Load ranking strategies from JSON file"""
        strategies_file = Path("configs/schemas/ranking_strategies.json")
        
        if not strategies_file.exists():
            # Auto-generate if missing
            from .ranking_strategy_extractor import extract_all_ranking_strategies, save_ranking_strategies
            
            schemas_dir = Path("configs/schemas")
            strategies = extract_all_ranking_strategies(schemas_dir)
            save_ranking_strategies(strategies, strategies_file)
            logger.info("Generated ranking strategies file")
        
        with open(strategies_file, 'r') as f:
            return json.load(f)
    
    def get_required_embeddings(self, schema_name: str) -> Dict[str, bool]:
        """Determine what embeddings are needed based on ranking strategies"""
        
        if schema_name not in self.ranking_strategies:
            raise ValueError(f"No ranking strategies found for schema '{schema_name}'")
        
        schema_strategies = self.ranking_strategies[schema_name]
        
        # Check what any strategy needs
        needs_float = any(
            s.get("needs_float_embeddings", False) 
            for s in schema_strategies.values()
        )
        needs_binary = any(
            s.get("needs_binary_embeddings", False) 
            for s in schema_strategies.values()
        )
        
        return {
            "needs_float": needs_float,
            "needs_binary": needs_binary
        }
    
    def get_embedding_field_names(self, schema_name: str) -> Dict[str, str]:
        """Get the actual embedding field names used by the schema"""
        
        if schema_name not in self.ranking_strategies:
            raise ValueError(f"No ranking strategies found for schema '{schema_name}'")
        
        schema_strategies = self.ranking_strategies[schema_name]
        
        # Collect unique field names from all strategies
        float_fields = set()
        binary_fields = set()
        
        for strategy in schema_strategies.values():
            if strategy.get("needs_float_embeddings", False):
                field = strategy.get("embedding_field")
                if field and "binary" not in field:
                    float_fields.add(field)
            
            if strategy.get("needs_binary_embeddings", False):
                field = strategy.get("embedding_field")
                if field and "binary" in field:
                    binary_fields.add(field)
        
        # Use the most common or first field name found
        result = {}
        if float_fields:
            result["float_field"] = next(iter(float_fields))
        if binary_fields:
            result["binary_field"] = next(iter(binary_fields))
        
        return result
    
    def get_tensor_info(self, schema_name: str) -> Dict[str, Any]:
        """Extract tensor dimension information from strategies"""
        
        schema_strategies = self.ranking_strategies[schema_name]
        tensor_info = {}
        
        # Look for tensor definitions in inputs
        for strategy in schema_strategies.values():
            if "query_tensor_name" in strategy:
                tensor_name = strategy["query_tensor_name"]
                
                # Find the tensor type from any strategy that uses it
                for s in schema_strategies.values():
                    for input_name, input_type in s.get("inputs", {}).items():
                        if tensor_name in input_name:
                            tensor_info[tensor_name] = self._parse_tensor_type(input_type)
        
        return tensor_info
    
    def _parse_tensor_type(self, tensor_type: str) -> Dict[str, Any]:
        """Parse tensor type string to extract dimensions"""
        # Example: "tensor<float>(querytoken{}, v[128])" â†’ {"type": "float", "dims": {"v": 128}}
        
        import re
        
        # Extract type (float, int8, etc.)
        type_match = re.search(r'tensor<(\w+)>', tensor_type)
        tensor_element_type = type_match.group(1) if type_match else "float"
        
        # Extract dimensions
        dims = {}
        
        # Indexed dimensions like v[128]
        indexed_dims = re.findall(r'(\w+)\[(\d+)\]', tensor_type)
        for dim_name, dim_size in indexed_dims:
            dims[dim_name] = int(dim_size)
        
        # Mapped dimensions like querytoken{}
        mapped_dims = re.findall(r'(\w+)\{\}', tensor_type)
        for dim_name in mapped_dims:
            dims[dim_name] = -1  # -1 indicates variable size
        
        return {
            "type": tensor_element_type,
            "dimensions": dims
        }
    
    def format_document(
        self, 
        schema_name: str,
        video_id: str,
        embeddings: Optional[np.ndarray],
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Format document based on ranking strategy requirements"""
        
        # Get what embeddings are needed
        requirements = self.get_required_embeddings(schema_name)
        
        # Start with base document
        document = {
            "video_id": video_id,
            **metadata  # frame_id, timestamps, etc.
        }
        
        # Add embeddings based on requirements
        if embeddings is not None:
            if requirements["needs_float"]:
                # Check if patch-based (2D) or global (1D)
                if embeddings.ndim == 2:
                    # Patch-based: format as cells
                    document["embedding"] = self._format_patch_embeddings(embeddings)
                else:
                    # Global: format as list
                    document["embedding"] = embeddings.tolist()
            
            if requirements["needs_binary"]:
                # Generate binary embeddings
                binary_embeddings = self._generate_binary_embeddings(embeddings)
                
                if binary_embeddings.ndim == 2:
                    document["embedding_binary"] = self._format_patch_embeddings(binary_embeddings)
                else:
                    document["embedding_binary"] = binary_embeddings.tolist()
        
        return document
    
    def _generate_binary_embeddings(self, embeddings: np.ndarray) -> np.ndarray:
        """Generate binary embeddings from float embeddings"""
        # Binarize (>0 becomes 1, <=0 becomes 0)
        binary = np.where(embeddings > 0, 1, 0).astype(np.uint8)
        
        # Pack bits
        if embeddings.ndim == 1:
            # 1D: pad and pack
            padding = (8 - len(binary) % 8) % 8
            if padding:
                binary = np.pad(binary, (0, padding), mode='constant')
            return np.packbits(binary).astype(np.int8)
        else:
            # 2D: pack each row
            return np.packbits(binary, axis=1).astype(np.int8)
    
    def _format_patch_embeddings(self, embeddings: np.ndarray) -> Dict[str, Any]:
        """Format patch-based embeddings as cells"""
        cells = []
        
        for patch_idx in range(embeddings.shape[0]):
            for v_idx in range(embeddings.shape[1]):
                cells.append({
                    "address": {
                        "patch": str(patch_idx),
                        "v": str(v_idx)
                    },
                    "value": float(embeddings[patch_idx, v_idx])
                })
        
        return {"cells": cells}


# Example usage in processing pipeline
def process_with_strategy_awareness(video_path: str, profile: str):
    """Example of how to use strategy-aware processing"""
    
    # Initialize processor
    processor = StrategyAwareProcessor()
    
    # Schema name is the same as profile name
    schema_name = profile
    
    # Check what's needed
    requirements = processor.get_required_embeddings(schema_name)
    print(f"Schema {schema_name} requires:")
    print(f"  Float embeddings: {requirements['needs_float']}")
    print(f"  Binary embeddings: {requirements['needs_binary']}")
    
    # Get tensor information
    tensor_info = processor.get_tensor_info(schema_name)
    print(f"  Tensor dimensions: {tensor_info}")
    
    # Process video and generate embeddings
    # ... (video processing code)
    embeddings = np.random.randn(16, 128)  # Example
    
    # Format document based on strategy requirements
    document = processor.format_document(
        schema_name=schema_name,
        video_id="test_video",
        embeddings=embeddings,
        metadata={
            "frame_id": 1,
            "start_time": 0.0,
            "end_time": 1.0
        }
    )
    
    print(f"Generated document fields: {list(document.keys())}")
    
    return document