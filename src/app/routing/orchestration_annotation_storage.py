"""
Orchestration Annotation Storage

Stores human annotations for orchestration workflow quality and corrections.
"""

import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional

import phoenix as px

logger = logging.getLogger(__name__)


@dataclass
class OrchestrationAnnotation:
    """
    Human annotation for an orchestration workflow

    Captures human feedback on:
    - Was the orchestration pattern optimal?
    - Were the right agents selected?
    - Was the execution order correct?
    - Overall workflow quality
    - Suggested improvements
    """

    # Workflow identification (required fields)
    workflow_id: str
    span_id: str
    query: str

    # Actual orchestration details (required fields)
    orchestration_pattern: str  # parallel, sequential, etc.
    agents_used: List[str]
    execution_order: List[str]
    execution_time: float

    # Human feedback - pattern optimality (required fields)
    pattern_is_optimal: bool

    # Human feedback - agent selection (required fields)
    agents_are_correct: bool

    # Human feedback - execution order (required fields)
    execution_order_is_optimal: bool

    # Human feedback - overall quality (required fields)
    workflow_quality_label: str  # excellent, good, acceptable, poor, failed
    quality_score: float  # 0.0-1.0

    # Annotation metadata (required fields)
    annotator_id: str

    # Optional fields with defaults
    suggested_pattern: Optional[str] = None
    pattern_feedback: Optional[str] = None
    missing_agents: List[str] = field(default_factory=list)
    unnecessary_agents: List[str] = field(default_factory=list)
    suggested_agents: List[str] = field(default_factory=list)
    suggested_execution_order: Optional[List[str]] = None
    execution_order_feedback: Optional[str] = None
    improvement_notes: Optional[str] = None
    what_went_well: Optional[str] = None
    what_went_wrong: Optional[str] = None
    annotation_timestamp: datetime = field(default_factory=datetime.now)
    annotation_source: str = "human"  # human, llm_auto, hybrid
    workflow_succeeded: bool = True
    error_details: Optional[str] = None


class OrchestrationAnnotationStorage:
    """
    Storage for orchestration workflow annotations

    Stores annotations in Phoenix as span evaluations with custom metadata.
    """

    def __init__(self, tenant_id: str = "default"):
        """Initialize annotation storage"""
        self.tenant_id = tenant_id
        self.phoenix_client = px.Client()

        logger.info(
            f"🗄️  Initialized OrchestrationAnnotationStorage for tenant '{tenant_id}'"
        )

    async def store_annotation(self, annotation: OrchestrationAnnotation) -> bool:
        """
        Store orchestration annotation in Phoenix

        Args:
            annotation: Orchestration annotation to store

        Returns:
            True if stored successfully, False otherwise
        """
        try:
            # Create evaluation record in Phoenix
            # This attaches annotation data to the orchestration span

            evaluation_data = {
                "span_id": annotation.span_id,
                "name": "orchestration_quality",
                "annotator_kind": annotation.annotation_source,
                "result": {
                    "label": annotation.workflow_quality_label,
                    "score": annotation.quality_score,
                },
                "metadata": {
                    # Pattern feedback
                    "pattern_is_optimal": annotation.pattern_is_optimal,
                    "suggested_pattern": annotation.suggested_pattern,
                    "pattern_feedback": annotation.pattern_feedback,
                    # Agent selection feedback
                    "agents_are_correct": annotation.agents_are_correct,
                    "missing_agents": ",".join(annotation.missing_agents),
                    "unnecessary_agents": ",".join(annotation.unnecessary_agents),
                    "suggested_agents": ",".join(annotation.suggested_agents),
                    # Execution order feedback
                    "execution_order_is_optimal": annotation.execution_order_is_optimal,
                    "suggested_execution_order": ",".join(
                        annotation.suggested_execution_order or []
                    ),
                    "execution_order_feedback": annotation.execution_order_feedback,
                    # Improvement notes
                    "improvement_notes": annotation.improvement_notes,
                    "what_went_well": annotation.what_went_well,
                    "what_went_wrong": annotation.what_went_wrong,
                    # Original workflow data
                    "actual_pattern": annotation.orchestration_pattern,
                    "actual_agents": ",".join(annotation.agents_used),
                    "actual_execution_order": ",".join(annotation.execution_order),
                    "execution_time": annotation.execution_time,
                    "workflow_succeeded": annotation.workflow_succeeded,
                    # Annotator
                    "annotator_id": annotation.annotator_id,
                    "annotation_timestamp": annotation.annotation_timestamp.isoformat(),
                },
            }

            # Store in Phoenix (API call)
            self.phoenix_client.log_evaluations([evaluation_data])

            logger.info(
                f"✅ Stored orchestration annotation for workflow {annotation.workflow_id}"
            )
            return True

        except Exception as e:
            logger.error(f"❌ Error storing annotation: {e}")
            return False

    async def query_annotated_spans(
        self,
        start_time: datetime,
        end_time: datetime,
        only_human_reviewed: bool = True,
        min_quality_score: Optional[float] = None,
    ) -> List[Dict[str, Any]]:
        """
        Query orchestration spans that have been annotated

        Args:
            start_time: Start of time range
            end_time: End of time range
            only_human_reviewed: Only return human annotations (not LLM auto-annotations)
            min_quality_score: Filter by minimum quality score

        Returns:
            List of annotated span data with annotations
        """
        try:
            # Query Phoenix for spans with orchestration_quality evaluations
            spans_df = self.phoenix_client.get_spans_dataframe(
                start_time=start_time,
                end_time=end_time,
            )

            # Filter for orchestration spans with evaluations
            annotated_spans = []

            for _, span_row in spans_df.iterrows():
                # Check if span has orchestration_quality evaluation
                evaluations = self._get_span_evaluations(span_row)

                if not evaluations:
                    continue

                # Filter by annotation source
                if only_human_reviewed:
                    evaluations = [
                        e for e in evaluations if e.get("annotator_kind") == "human"
                    ]

                # Filter by quality score
                if min_quality_score is not None:
                    evaluations = [
                        e
                        for e in evaluations
                        if e.get("result", {}).get("score", 0.0) >= min_quality_score
                    ]

                if evaluations:
                    annotated_spans.append(
                        {
                            "span_id": span_row.get("context.span_id"),
                            "span_data": span_row.to_dict(),
                            "annotations": evaluations,
                        }
                    )

            logger.info(
                f"📊 Found {len(annotated_spans)} annotated orchestration spans"
            )
            return annotated_spans

        except Exception as e:
            logger.error(f"❌ Error querying annotated spans: {e}")
            return []

    def _get_span_evaluations(self, span_row) -> List[Dict]:
        """
        Extract evaluations for a span

        Note: This uses Phoenix API to get evaluations attached to spans.
        The actual implementation depends on Phoenix client capabilities.
        """
        try:
            span_id = span_row.get("context.span_id")
            if not span_id:
                return []

            # Get evaluations for this span from Phoenix client
            evals_df = self.phoenix_client.get_evaluations(span_ids=[span_id])

            if evals_df.empty:
                return []

            # Filter for orchestration_quality evaluations
            orch_evals = evals_df[evals_df["name"] == "orchestration_quality"]

            evaluations = []
            for _, eval_row in orch_evals.iterrows():
                evaluations.append(
                    {
                        "annotator_kind": eval_row.get("annotator_kind", "unknown"),
                        "result": {
                            "label": eval_row.get("label"),
                            "score": eval_row.get("score", 0.0),
                        },
                        "metadata": eval_row.get("metadata", {}),
                    }
                )

            return evaluations

        except Exception as e:
            logger.error(f"Error extracting evaluations for span {span_id}: {e}")
            raise
