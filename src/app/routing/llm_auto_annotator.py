"""
LLM Auto-Annotation Agent for Routing Decisions

Uses LLM to analyze routing spans and provide initial annotations:
- Analyzes routing decision context
- Reviews downstream execution results
- Labels decisions as correct/incorrect/ambiguous
- Provides reasoning for annotations
"""

import json
import logging
import os
from dataclasses import dataclass
from enum import Enum
from typing import Optional

from litellm import completion

from src.app.routing.annotation_agent import AnnotationRequest

logger = logging.getLogger(__name__)


class AnnotationLabel(Enum):
    """Labels for routing decisions"""
    CORRECT_ROUTING = "correct_routing"  # Right agent chosen
    WRONG_ROUTING = "wrong_routing"  # Wrong agent chosen
    AMBIGUOUS = "ambiguous"  # Multiple agents could work
    INSUFFICIENT_INFO = "insufficient_info"  # Cannot determine


@dataclass
class AutoAnnotation:
    """Automatic annotation generated by LLM"""
    span_id: str
    label: AnnotationLabel
    confidence: float  # LLM's confidence in annotation (0-1)
    reasoning: str
    suggested_correct_agent: Optional[str]  # If wrong_routing, what should it be
    requires_human_review: bool  # True if LLM is uncertain


class LLMAutoAnnotator:
    """
    LLM-based agent that provides initial annotations on routing decisions

    Uses LiteLLM to call configured model to analyze:
    - The original query and context
    - The routing decision made (chosen agent + confidence)
    - The downstream execution results
    - Any error messages or failure indicators

    Outputs structured annotation with reasoning
    """

    def __init__(self, model: Optional[str] = None, api_base: Optional[str] = None):
        """
        Initialize LLM auto-annotator

        Args:
            model: Model name to use (defaults to env var or claude-3-5-sonnet-20241022)
            api_base: API base URL for local models (e.g., http://localhost:11434 for Ollama)
        """
        self.model = model or os.getenv("ANNOTATION_MODEL", "claude-3-5-sonnet-20241022")
        self.api_base = api_base or os.getenv("ANNOTATION_API_BASE")

        logger.info(
            f"ü§ñ Initialized LLMAutoAnnotator with model: {self.model}"
            + (f", api_base: {self.api_base}" if self.api_base else "")
        )

    def annotate(self, annotation_request: AnnotationRequest) -> AutoAnnotation:
        """
        Generate automatic annotation for a routing decision

        Args:
            annotation_request: Request containing span data to annotate

        Returns:
            AutoAnnotation with label, confidence, and reasoning
        """
        logger.info(f"ü§ñ Auto-annotating span {annotation_request.span_id}")

        # Build prompt for LLM
        prompt = self._build_annotation_prompt(annotation_request)

        try:
            # Call LLM via LiteLLM
            kwargs = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 1024,
                "temperature": 0.3  # Lower temperature for more consistent annotations
            }

            if self.api_base:
                kwargs["api_base"] = self.api_base

            response = completion(**kwargs)

            # Parse response
            response_text = response.choices[0].message.content
            annotation = self._parse_llm_response(
                response_text,
                annotation_request.span_id
            )

            logger.info(
                f"‚úÖ Generated annotation: {annotation.label.value} "
                f"(confidence: {annotation.confidence:.2f}, "
                f"review_needed: {annotation.requires_human_review})"
            )

            return annotation

        except Exception as e:
            logger.error(f"‚ùå Error generating annotation: {e}")
            # Return fallback annotation
            return AutoAnnotation(
                span_id=annotation_request.span_id,
                label=AnnotationLabel.INSUFFICIENT_INFO,
                confidence=0.0,
                reasoning=f"Error during annotation: {str(e)}",
                suggested_correct_agent=None,
                requires_human_review=True
            )

    def _build_annotation_prompt(self, request: AnnotationRequest) -> str:
        """
        Build prompt for LLM annotation

        Args:
            request: Annotation request with span data

        Returns:
            Formatted prompt string
        """
        # Extract context details
        outcome_details = request.context.get("outcome_details", {})
        span_status = request.context.get("span_status", "UNKNOWN")
        routing_context = request.context.get("routing_context", {})

        prompt = f"""You are an expert at evaluating routing decisions in a multi-agent system.

Your task is to analyze a routing decision and determine if the correct agent was chosen.

## Query Information
- **Query**: {request.query}
- **Query Context**: {json.dumps(routing_context, indent=2) if routing_context else "None"}

## Routing Decision Made
- **Chosen Agent**: {request.chosen_agent}
- **Routing Confidence**: {request.routing_confidence:.2f}

## Execution Results
- **Outcome**: {request.outcome.value}
- **Span Status**: {span_status}
- **Outcome Details**: {json.dumps(outcome_details, indent=2)}

## Your Task
Analyze whether the routing decision was correct based on:
1. The query content and intent
2. The agent that was chosen
3. The execution results (did it succeed or fail?)

Provide your annotation in JSON format:
{{
    "label": "correct_routing | wrong_routing | ambiguous | insufficient_info",
    "confidence": 0.0-1.0,
    "reasoning": "Explain why you chose this label in 1-2 sentences",
    "suggested_correct_agent": "agent_name or null (only if label is wrong_routing)",
    "requires_human_review": true/false (true if you're uncertain)
}}

Guidelines:
- "correct_routing": The chosen agent was appropriate for the query and execution was successful
- "wrong_routing": A different agent should have been chosen (suggest which one)
- "ambiguous": Multiple agents could handle this query, or results are unclear
- "insufficient_info": Cannot determine correctness from available information

Be conservative - if unsure, mark requires_human_review as true."""

        return prompt

    def _parse_llm_response(self, response_text: str, span_id: str) -> AutoAnnotation:
        """
        Parse LLM response into AutoAnnotation

        Args:
            response_text: Raw LLM response
            span_id: Span ID being annotated

        Returns:
            AutoAnnotation object
        """
        try:
            # Try to extract JSON from response
            # LLM might wrap JSON in markdown code blocks
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
            else:
                json_text = response_text.strip()

            # Parse JSON
            data = json.loads(json_text)

            # Convert label string to enum
            label_str = data.get("label", "insufficient_info")
            try:
                label = AnnotationLabel(label_str)
            except ValueError:
                logger.warning(f"‚ö†Ô∏è Invalid label '{label_str}', using INSUFFICIENT_INFO")
                label = AnnotationLabel.INSUFFICIENT_INFO

            return AutoAnnotation(
                span_id=span_id,
                label=label,
                confidence=float(data.get("confidence", 0.5)),
                reasoning=data.get("reasoning", "No reasoning provided"),
                suggested_correct_agent=data.get("suggested_correct_agent"),
                requires_human_review=bool(data.get("requires_human_review", True))
            )

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Failed to parse LLM response as JSON: {e}")
            logger.debug(f"Response text: {response_text}")
            return AutoAnnotation(
                span_id=span_id,
                label=AnnotationLabel.INSUFFICIENT_INFO,
                confidence=0.0,
                reasoning=f"Failed to parse LLM response: {str(e)}",
                suggested_correct_agent=None,
                requires_human_review=True
            )

        except Exception as e:
            logger.error(f"‚ùå Error parsing LLM response: {e}")
            return AutoAnnotation(
                span_id=span_id,
                label=AnnotationLabel.INSUFFICIENT_INFO,
                confidence=0.0,
                reasoning=f"Error parsing response: {str(e)}",
                suggested_correct_agent=None,
                requires_human_review=True
            )

    def batch_annotate(self, requests: list[AnnotationRequest]) -> list[AutoAnnotation]:
        """
        Generate annotations for multiple requests

        Args:
            requests: List of annotation requests

        Returns:
            List of auto-annotations
        """
        logger.info(f"ü§ñ Starting batch annotation of {len(requests)} spans")

        annotations = []
        for request in requests:
            try:
                annotation = self.annotate(request)
                annotations.append(annotation)
            except Exception as e:
                logger.error(f"‚ùå Error annotating span {request.span_id}: {e}")
                # Add error annotation
                annotations.append(
                    AutoAnnotation(
                        span_id=request.span_id,
                        label=AnnotationLabel.INSUFFICIENT_INFO,
                        confidence=0.0,
                        reasoning=f"Batch annotation error: {str(e)}",
                        suggested_correct_agent=None,
                        requires_human_review=True
                    )
                )

        logger.info(f"‚úÖ Completed batch annotation: {len(annotations)} annotations generated")

        return annotations
