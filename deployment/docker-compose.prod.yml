# Cogniverse Multi-Agent RAG System - Production Configuration
# Docker Compose for production deployment with enhanced security and resource management

version: '3.8'

services:
  # Vespa Search Backend
  vespa:
    image: vespaengine/vespa:8.259.19  # Pinned version for production
    container_name: cogniverse-vespa-prod
    ports:
      - "8080:8080"  # Query/Document API
      - "19071:19071"  # Config API
      - "19092:19092"  # Metrics
    volumes:
      - vespa-data:/opt/vespa/var
    environment:
      - VESPA_CONFIGSERVERS=vespa
      - VESPA_MEMORY_OPTIONS=-Xms4g -Xmx16g
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 20G
        reservations:
          cpus: '4'
          memory: 8G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19071/state/v1/health"]
      interval: 20s
      timeout: 5s
      retries: 3
      start_period: 120s
    networks:
      - cogniverse
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Cogniverse Runtime (Unified FastAPI)
  runtime:
    build:
      context: ..
      dockerfile: libs/runtime/Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: cogniverse/runtime:${VERSION:-latest}
    container_name: cogniverse-runtime-prod
    ports:
      - "8000:8000"
    volumes:
      - ../configs/config.json:/app/configs/config.json:ro
      - runtime-data:/data
      - runtime-logs:/logs
    env_file:
      - .env.prod
    environment:
      - RUNTIME_HOST=0.0.0.0
      - RUNTIME_PORT=8000
      - VESPA_URL=http://vespa
      - VESPA_PORT=8080
      - VESPA_CONFIG_PORT=19071
      - PHOENIX_ENDPOINT=http://phoenix:6006
      - OLLAMA_ENDPOINT=http://ollama:11434
      - PYTHONUNBUFFERED=1
      - OTLP_ENDPOINT=http://phoenix:4317
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - WORKERS=${RUNTIME_WORKERS:-4}
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
      replicas: ${RUNTIME_REPLICAS:-2}
    depends_on:
      vespa:
        condition: service_healthy
      phoenix:
        condition: service_started
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s
    networks:
      - cogniverse
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Cogniverse Dashboard (Streamlit)
  dashboard:
    build:
      context: ..
      dockerfile: libs/dashboard/Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: cogniverse/dashboard:${VERSION:-latest}
    container_name: cogniverse-dashboard-prod
    ports:
      - "8501:8501"
    volumes:
      - ../configs/config.json:/app/configs/config.json:ro
      - runtime-data:/data:ro
      - runtime-logs:/app/outputs:ro
    env_file:
      - .env.prod
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_ENABLE_CORS=false
      - STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION=true
      - RUNTIME_URL=http://runtime:8000
      - PHOENIX_ENDPOINT=http://phoenix:6006
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    depends_on:
      - runtime
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - cogniverse
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Phoenix (Arize - for evaluation and observability)
  phoenix:
    image: arizephoenix/phoenix:4.11.0  # Pinned version for production
    container_name: cogniverse-phoenix-prod
    ports:
      - "6006:6006"   # UI + REST API
      - "4317:4317"   # OTLP gRPC collector
    volumes:
      - phoenix-data:/phoenix-data
    environment:
      - PHOENIX_WORKING_DIR=/phoenix-data
      - PHOENIX_PORT=6006
      - PHOENIX_HOST=0.0.0.0
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - cogniverse
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Ollama (Local LLM inference)
  ollama:
    image: ollama/ollama:0.1.22  # Pinned version for production
    container_name: cogniverse-ollama-prod
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=http://runtime:8000,http://localhost:8000
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-2}
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '4'
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - cogniverse
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Nginx Reverse Proxy (Optional - for SSL termination and load balancing)
  nginx:
    image: nginx:1.25-alpine
    container_name: cogniverse-nginx-prod
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ../configs/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ../configs/nginx/conf.d:/etc/nginx/conf.d:ro
      - ../configs/nginx/ssl:/etc/nginx/ssl:ro
      - nginx-logs:/var/log/nginx
    depends_on:
      - runtime
      - dashboard
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - cogniverse
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    profiles:
      - with-nginx

networks:
  cogniverse:
    driver: bridge
    name: cogniverse-network-prod
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  vespa-data:
    driver: local
  phoenix-data:
    driver: local
  ollama-data:
    driver: local
  runtime-data:
    driver: local
  runtime-logs:
    driver: local
  nginx-logs:
    driver: local

# Production Usage:
#
# 1. Create production environment file:
#    cp deployment/.env.prod.example deployment/.env.prod
#    # Edit .env.prod with production values
#
# 2. Start all services:
#    docker compose -f deployment/docker-compose.prod.yml up -d
#
# 3. Start with Nginx reverse proxy:
#    docker compose -f deployment/docker-compose.prod.yml --profile with-nginx up -d
#
# 4. Scale runtime instances:
#    docker compose -f deployment/docker-compose.prod.yml up -d --scale runtime=4
#
# 5. View logs:
#    docker compose -f deployment/docker-compose.prod.yml logs -f runtime
#
# 6. Monitor services:
#    docker compose -f deployment/docker-compose.prod.yml ps
#    docker stats
#
# 7. Health checks:
#    curl http://localhost:8000/health
#    curl http://localhost:8501/_stcore/health
#    curl http://localhost:6006/health
#
# 8. Rolling updates:
#    docker compose -f deployment/docker-compose.prod.yml pull
#    docker compose -f deployment/docker-compose.prod.yml up -d --no-deps --build runtime
#
# 9. Backup volumes:
#    docker run --rm -v cogniverse_vespa-data:/data -v $(pwd)/backups:/backup alpine tar czf /backup/vespa-$(date +%Y%m%d).tar.gz /data
#
# 10. Stop all services:
#     docker compose -f deployment/docker-compose.prod.yml down
#
# 11. Stop and remove volumes (CAUTION - data loss):
#     docker compose -f deployment/docker-compose.prod.yml down -v
