# Cogniverse Study Guide: Scripts & Operations Module

**Last Updated:** 2025-10-07
**Module Path:** `scripts/`
**Purpose:** Operational scripts for system deployment, ingestion, optimization, experimentation, and management

---

## Table of Contents
1. [Module Overview](#module-overview)
2. [Architecture](#architecture)
3. [Core Scripts](#core-scripts)
4. [Data Flow](#data-flow)
5. [Usage Examples](#usage-examples)
6. [Production Considerations](#production-considerations)

---

## Module Overview

### Purpose
The Scripts & Operations module provides command-line tools for:
- **Video Ingestion**: Processing and indexing video content
- **Schema Deployment**: Managing Vespa search schemas
- **System Setup**: Initializing the system environment
- **Optimization**: Running DSPy optimization workflows
- **Experimentation**: Conducting Phoenix experiments with visualization
- **Dataset Management**: Managing evaluation datasets
- **Dashboard**: Interactive Streamlit-based analytics UI

### Key Features
- **Builder Pattern Ingestion**: Fluent API for configurable pipeline construction
- **Multi-Profile Support**: Process videos with multiple embedding strategies simultaneously
- **Async Processing**: Concurrent video processing with configurable limits
- **Phoenix Integration**: Experiment tracking with visual analytics
- **Schema Management**: JSON-based schema deployment and validation
- **Interactive Dashboards**: Streamlit tabs for analytics, configuration, and memory management

### Script Categories

```
scripts/
‚îú‚îÄ‚îÄ Ingestion & Processing
‚îÇ   ‚îú‚îÄ‚îÄ run_ingestion.py              # Main video ingestion pipeline
‚îÇ   ‚îú‚îÄ‚îÄ test_ingestion.py             # Test ingestion with validation
‚îÇ   ‚îú‚îÄ‚îÄ ingest_documents.py           # Document ingestion
‚îÇ   ‚îú‚îÄ‚îÄ ingest_images.py              # Image ingestion
‚îÇ   ‚îî‚îÄ‚îÄ ingest_audio.py               # Audio ingestion
‚îÇ
‚îú‚îÄ‚îÄ Deployment & Setup
‚îÇ   ‚îú‚îÄ‚îÄ deploy_json_schema.py         # Deploy single JSON schema
‚îÇ   ‚îú‚îÄ‚îÄ deploy_all_schemas.py         # Deploy all schemas at once
‚îÇ   ‚îú‚îÄ‚îÄ deploy_memory_schema.py       # Deploy memory schema
‚îÇ   ‚îú‚îÄ‚îÄ setup_system.py               # System initialization
‚îÇ   ‚îú‚îÄ‚îÄ setup_ollama.py               # Ollama model setup
‚îÇ   ‚îú‚îÄ‚îÄ setup_gliner.py               # GLiNER setup
‚îÇ   ‚îî‚îÄ‚îÄ setup_video_processing.py    # Video processing setup
‚îÇ
‚îú‚îÄ‚îÄ Optimization & Experiments
‚îÇ   ‚îú‚îÄ‚îÄ run_optimization.py           # DSPy optimization workflow
‚îÇ   ‚îú‚îÄ‚îÄ run_experiments_with_visualization.py  # Phoenix experiments
‚îÇ   ‚îú‚îÄ‚îÄ optimize_system.py            # System-wide optimization
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_phase5.py          # Optimization checkpoint
‚îÇ
‚îú‚îÄ‚îÄ Dataset Management
‚îÇ   ‚îú‚îÄ‚îÄ manage_datasets.py            # Dataset CRUD operations
‚îÇ   ‚îú‚îÄ‚îÄ create_golden_dataset_from_traces.py  # Golden dataset from traces
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap_dataset_from_traces.py      # Bootstrap from traces
‚îÇ   ‚îú‚îÄ‚îÄ generate_dataset_from_videos.py       # Dataset from videos
‚îÇ   ‚îú‚îÄ‚îÄ create_sample_dataset.py      # Sample dataset creation
‚îÇ   ‚îî‚îÄ‚îÄ interactive_dataset_builder.py  # Interactive builder UI
‚îÇ
‚îú‚îÄ‚îÄ Dashboard & UI
‚îÇ   ‚îú‚îÄ‚îÄ phoenix_dashboard_standalone.py  # Main analytics dashboard
‚îÇ   ‚îú‚îÄ‚îÄ config_management_tab.py      # Config management UI
‚îÇ   ‚îú‚îÄ‚îÄ memory_management_tab.py      # Memory management UI
‚îÇ   ‚îú‚îÄ‚îÄ embedding_atlas_tab.py        # Embedding visualization
‚îÇ   ‚îú‚îÄ‚îÄ routing_evaluation_tab.py     # Routing evaluation UI
‚îÇ   ‚îî‚îÄ‚îÄ orchestration_annotation_tab.py  # Orchestration annotation
‚îÇ
‚îî‚îÄ‚îÄ Utilities & Analysis
    ‚îú‚îÄ‚îÄ analyze_traces.py             # Phoenix trace analysis
    ‚îú‚îÄ‚îÄ export_vespa_embeddings.py    # Export embeddings
    ‚îú‚îÄ‚îÄ export_backend_embeddings.py  # Backend embedding export
    ‚îî‚îÄ‚îÄ manage_phoenix_data.py        # Phoenix data management
```

---

## Architecture

### 1. Ingestion Pipeline Architecture

```mermaid
graph TB
    Entry[run_ingestion.py<br/>Command Line Entry Point]

    TestMode[Test Mode<br/>build_test_pipeline]
    SimpleMode[Simple Mode<br/>build_simple_pipeline]
    AdvMode[Advanced Mode<br/>create_pipeline]

    Pipeline[IngestionPipeline<br/>‚Ä¢ Video Processing<br/>‚Ä¢ Embedding Generation<br/>‚Ä¢ Vespa Upload]

    ColPali[ColPali Profile<br/>Frame-based]
    VideoPrism[VideoPrism Profile<br/>Global embeddings]
    ColQwen[ColQwen Profile<br/>Chunk-based]

    Concurrent[Concurrent Async<br/>Video Processing<br/>max_concurrent=3]

    Vespa[Vespa Backend<br/>Bulk Upload]

    Entry --> TestMode
    Entry --> SimpleMode
    Entry --> AdvMode

    TestMode --> Pipeline
    SimpleMode --> Pipeline
    AdvMode --> Pipeline

    Pipeline --> ColPali
    Pipeline --> VideoPrism
    Pipeline --> ColQwen

    ColPali --> Concurrent
    VideoPrism --> Concurrent
    ColQwen --> Concurrent

    Concurrent --> Vespa

    style Entry fill:#e1f5ff
    style Pipeline fill:#fff4e1
    style Concurrent fill:#fff4e1
    style Vespa fill:#e1ffe1
```

### 2. Optimization Workflow Architecture

```mermaid
graph TB
    Start[run_optimization.py<br/>Complete Optimization & Deployment]

    Step1[Step 1: Run Orchestrator<br/>‚Ä¢ Execute optimization<br/>‚Ä¢ Generate prompt artifacts<br/>‚Ä¢ Timeout: 2 hours]

    Step2[Step 2: Upload to Modal<br/>‚Ä¢ Upload artifacts to Modal volume<br/>‚Ä¢ Path: /artifacts/*.json]

    Step3[Step 3: Deploy Production API<br/>‚Ä¢ Deploy to Modal<br/>‚Ä¢ Setup HuggingFace secret<br/>‚Ä¢ Return API URL]

    Step4[Step 4: Test Production API<br/>‚Ä¢ Run test cases<br/>‚Ä¢ Verify modality routing<br/>‚Ä¢ Check generation types]

    Start --> Step1
    Step1 --> Step2
    Step2 --> Step3
    Step3 --> Step4

    style Start fill:#e1f5ff
    style Step1 fill:#fff4e1
    style Step2 fill:#fff4e1
    style Step3 fill:#fff4e1
    style Step4 fill:#e1ffe1
```

### 3. Experiment Workflow Architecture

```mermaid
graph TB
    Start[run_experiments_with_visualization.py<br/>Phoenix Experiment Runner]

    Runner[PhoenixExperimentRunner<br/>‚Ä¢ Experiment project isolation<br/>‚Ä¢ Quality evaluators optional<br/>‚Ä¢ LLM evaluators optional]

    Dataset[Dataset Preparation<br/>‚Ä¢ Load or create dataset<br/>‚Ä¢ CSV parsing<br/>‚Ä¢ Phoenix dataset registration]

    Loop[Multi-Profile Multi-Strategy Loop<br/>FOR each profile:<br/>  FOR each strategy:<br/>    ‚Ä¢ Run experiment<br/>    ‚Ä¢ Track spans<br/>    ‚Ä¢ Evaluate results<br/>    ‚Ä¢ Store metrics]

    Viz[Visualization Generation<br/>‚Ä¢ Profile summary table<br/>‚Ä¢ Strategy comparison<br/>‚Ä¢ Detailed results<br/>‚Ä¢ HTML report optional]

    Export[Results Export<br/>‚Ä¢ CSV summary<br/>‚Ä¢ JSON detailed results<br/>‚Ä¢ Phoenix UI links]

    Start --> Runner
    Runner --> Dataset
    Dataset --> Loop
    Loop --> Viz
    Viz --> Export

    style Start fill:#e1f5ff
    style Runner fill:#fff4e1
    style Dataset fill:#fff4e1
    style Loop fill:#fff4e1
    style Viz fill:#fff4e1
    style Export fill:#e1ffe1
```

### 4. Schema Deployment Architecture

```mermaid
graph TB
    subgraph Single["Single Schema Deployment"]
        Single1[deploy_json_schema.py<br/>Single Schema Deployment]
        Parser[JsonSchemaParser<br/>‚Ä¢ Load JSON schema file<br/>‚Ä¢ Parse to Vespa Schema object<br/>‚Ä¢ Validate structure]
        Package1[ApplicationPackage<br/>‚Ä¢ Create package<br/>‚Ä¢ Add schema<br/>‚Ä¢ Generate ZIP]
        Deploy1[HTTP Deployment<br/>‚Ä¢ POST to config server<br/>‚Ä¢ Port: 19071<br/>‚Ä¢ Endpoint: prepareandactivate]
        Verify[Verification<br/>‚Ä¢ Check ApplicationStatus<br/>‚Ä¢ Verify Vespa responding]

        Single1 --> Parser
        Parser --> Package1
        Package1 --> Deploy1
        Deploy1 --> Verify
    end

    subgraph Multi["Multi-Schema Deployment"]
        Multi1[deploy_all_schemas.py<br/>Multi-Schema Deployment]
        Discovery[Schema Discovery<br/>‚Ä¢ Scan configs/schemas/*.json<br/>‚Ä¢ Load all schema files]
        Package2[ApplicationPackage Multi-Schema<br/>‚Ä¢ Create single package<br/>‚Ä¢ Add all schemas<br/>‚Ä¢ Add validation overrides]
        Extract[Ranking Strategy Extraction<br/>‚Ä¢ Extract from all schemas<br/>‚Ä¢ Save to ranking_strategies.json]

        Multi1 --> Discovery
        Discovery --> Package2
        Package2 --> Extract
    end

    style Single1 fill:#e1f5ff
    style Multi1 fill:#e1f5ff
    style Parser fill:#fff4e1
    style Package1 fill:#fff4e1
    style Discovery fill:#fff4e1
    style Package2 fill:#fff4e1
    style Deploy1 fill:#ffe1f5
    style Verify fill:#e1ffe1
    style Extract fill:#e1ffe1
```

---

## Core Scripts

### 1. run_ingestion.py

**Purpose:** Main entry point for video ingestion pipeline with builder pattern configuration

**Location:** `scripts/run_ingestion.py` (205 lines)

**Command Line Arguments:**
```python
--video_dir PATH         # Directory containing videos
--output_dir PATH        # Output directory for processed data
--backend {byaldi,vespa} # Search backend (default: vespa)
--profile PROFILES       # Processing profiles (space-separated)
--max-concurrent INT     # Max concurrent videos (default: 3)
--max-frames INT         # Maximum frames per video
--test-mode             # Use test mode with limited frames
--debug                 # Enable debug mode
```

**Usage Modes:**

**Test Mode** (for quick validation):
```python
# Use test pipeline builder
pipeline = build_test_pipeline(
    video_dir=Path("data/testset/evaluation/sample_videos"),
    schema="video_colpali_smol500_mv_frame",
    max_frames=10
)
```

**Simple Mode** (for standard usage):
```python
# Use simple pipeline builder
pipeline = build_simple_pipeline(
    video_dir=Path("data/testset/evaluation/sample_videos"),
    schema="video_colpali_smol500_mv_frame",
    backend="vespa",
    debug=False
)
```

**Advanced Mode** (for custom configuration):
```python
# Use fluent builder with custom config
config = (create_config()
    .video_dir(Path("data/videos"))
    .backend("vespa")
    .output_dir(Path("custom/output"))
    .max_frames_per_video(100)
    .build())

pipeline = (create_pipeline()
    .with_config(config)
    .with_schema("video_colpali_smol500_mv_frame")
    .with_debug(True)
    .with_concurrency(5)
    .build())
```

**Multi-Profile Processing:**
```python
# Process with multiple profiles simultaneously
for profile in ["video_colpali_smol500_mv_frame",
                "video_videoprism_base_mv_chunk_30s"]:
    pipeline = build_simple_pipeline(
        video_dir=video_dir,
        schema=profile,
        backend="vespa"
    )
    results = await pipeline.process_videos_concurrent(
        video_files,
        max_concurrent=3
    )
```

**Output:**
- Success/failure status per video
- Documents fed to Vespa
- Processing time and throughput
- Per-profile summary statistics

---

### 2. deploy_json_schema.py

**Purpose:** Deploy individual JSON schema files to Vespa

**Location:** `scripts/deploy_json_schema.py` (198 lines)

**Command Line Arguments:**
```python
schema_file              # Path to JSON schema file (required)
--config-host HOST       # Vespa config server host (default: localhost)
--config-port PORT       # Config server port (default: 19071)
--data-host HOST         # Vespa data endpoint host (default: localhost)
--data-port PORT         # Data endpoint port (default: 8080)
```

**Deployment Process:**
```python
def deploy_json_schema(schema_file, vespa_host, config_port, data_port):
    # 1. Load JSON schema
    with open(schema_path, 'r') as f:
        schema_config = json.load(f)

    # 2. Parse schema using JsonSchemaParser
    parser = JsonSchemaParser()
    schema = parser.parse_schema(schema_config)

    # 3. Create application package
    app_package = ApplicationPackage(name=schema_name)
    app_package.add_schema(schema)

    # 4. Generate ZIP package
    app_zip = app_package.to_zip()

    # 5. Deploy via HTTP
    response = requests.post(
        f"http://{vespa_host}:{config_port}/application/v2/tenant/default/prepareandactivate",
        headers={"Content-Type": "application/zip"},
        data=app_zip,
        timeout=60
    )

    # 6. Verify deployment
    verify_deployment(schema_name, vespa_host, data_port)
```

**Verification:**
```python
def verify_deployment(schema_name, vespa_host, data_port):
    # Check application status endpoint
    response = requests.get(
        f"http://{vespa_host}:{data_port}/ApplicationStatus",
        timeout=5
    )
    return response.status_code == 200
```

**Example Usage:**
```bash
# Deploy agent memories schema
python scripts/deploy_json_schema.py configs/schemas/agent_memories_schema.json

# Deploy to remote Vespa instance
python scripts/deploy_json_schema.py configs/schemas/config_metadata_schema.json \
  --config-host vespa.example.com --config-port 19071
```

---

### 3. deploy_all_schemas.py

**Purpose:** Deploy all schemas from configs/schemas directory in a single application package

**Location:** `scripts/deploy_all_schemas.py` (104 lines)

**Deployment Workflow:**
```python
def main():
    # 1. Get all schema files
    schemas_dir = Path("configs/schemas")
    schema_files = list(schemas_dir.glob("*.json"))

    # 2. Create application package
    app_package = ApplicationPackage(name="videosearch")

    # 3. Parse and add each schema
    for schema_file in schema_files:
        parser = JsonSchemaParser()
        schema = parser.load_schema_from_json_file(str(schema_file))
        app_package.add_schema(schema)

    # 4. Add validation overrides (allow schema removal)
    until_date = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
    validation = Validation(
        validation_id="schema-removal",
        until=until_date
    )
    app_package.validations.append(validation)

    # 5. Deploy all schemas at once
    schema_manager._deploy_package(app_package)

    # 6. Extract ranking strategies from all schemas
    strategies = extract_all_ranking_strategies(schemas_dir)
    save_ranking_strategies(strategies, schemas_dir / "ranking_strategies.json")
```

**Benefits:**
- Single deployment for all schemas
- Consistent schema versions
- Automatic ranking strategy extraction
- Validation override handling

---

### 4. setup_system.py

**Purpose:** Initialize the system environment, create directories, and setup initial content

**Location:** `scripts/setup_system.py` (258 lines)

**Setup Steps:**
```python
def main():
    # 1. Check dependencies
    required_modules = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("byaldi", "Byaldi"),
        ("colpali_engine", "ColPali Engine"),
        ("faster_whisper", "Faster Whisper"),
        ("PIL", "Pillow"),
        ("cv2", "OpenCV")
    ]

    # 2. Create directories
    directories = [
        "data/videos",
        "data/text",
        "data/indexes",
        ".byaldi"
    ]

    # 3. Create sample content
    create_sample_content()  # README files
    download_sample_videos()  # Test video with imageio

    # 4. Setup video index
    setup_byaldi_index()  # Run ingestion pipeline
```

**Sample Video Creation:**
```python
def download_sample_videos():
    # Create a simple test video using imageio
    writer = imageio.get_writer('sample_test_video.mp4', fps=30)

    for i in range(90):  # 3 seconds at 30fps
        frame = np.zeros((480, 640, 3), dtype=np.uint8)
        x_pos = int((i / 90) * 500 + 50)
        frame[200:280, x_pos:x_pos+80] = [255, 0, 0]  # Red square

        # Simulate text
        if i % 30 < 15:
            frame[100:120, 50:590] = [255, 255, 255]

        writer.append_data(frame)

    writer.close()
```

**Next Steps After Setup:**
```bash
# 1. Start the servers
./scripts/run_servers.sh

# 2. Open browser
http://localhost:8000

# 3. Try example queries
"Show me videos with moving objects"
"Find clips from the test video"
```

---

### 5. run_optimization.py

**Purpose:** Complete optimization and deployment workflow for agentic router

**Location:** `scripts/run_optimization.py` (299 lines)

**Workflow Steps:**

**Step 1: Run Orchestrator**
```python
def run_orchestrator(config_path="config.json"):
    # Run orchestrator using new structure
    cmd = [sys.executable, "-m", "src.optimizer.orchestrator",
           "--config", config_path]

    result = subprocess.run(
        cmd,
        cwd=Path(__file__).parent.parent,
        capture_output=True,
        text=True,
        timeout=7200  # 2 hour timeout
    )

    # Find artifacts file
    artifacts_path = Path("optimization_results/unified_router_prompt_artifact.json")
    return str(artifacts_path)
```

**Step 2: Upload Artifacts to Modal**
```python
def upload_artifacts_to_modal(artifacts_path):
    # Create Modal volume and upload artifacts
    cmd = [
        "modal", "volume", "put",
        "optimization-artifacts",  # Volume name
        artifacts_path,
        "/artifacts/unified_router_prompt_artifact.json"
    ]

    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
    return result.returncode == 0
```

**Step 3: Deploy Production API**
```python
def deploy_production_api():
    # Check if HuggingFace secret exists
    result = subprocess.run(["modal", "secret", "list"],
                           capture_output=True, text=True)

    if "huggingface-token" not in result.stdout:
        # Create secret from environment
        hf_token = os.getenv("HF_TOKEN")
        subprocess.run(["modal", "secret", "create",
                       "huggingface-token", f"HF_TOKEN={hf_token}"])

    # Deploy to Modal
    cmd = ["modal", "deploy", "src/inference/modal_inference_service.py"]
    result = subprocess.run(cmd, timeout=600)

    return "https://agentic-router-production-route.modal.run"
```

**Step 4: Test Production API**
```python
def test_production_api(api_url):
    test_cases = [
        {
            "query": "Show me how to cook pasta",
            "expected_modality": "video",
            "expected_type": "raw_results"
        },
        {
            "query": "Create a detailed report on climate change",
            "expected_modality": "text",
            "expected_type": "detailed_report"
        }
    ]

    for test_case in test_cases:
        response = requests.post(
            api_url,
            json={"user_query": test_case["query"]},
            timeout=30
        )
        # Verify response format and routing decisions
```

**Command Line Options:**
```bash
python scripts/run_optimization.py \
  --config config.json \
  --skip-upload    # Skip uploading to Modal
  --skip-deploy    # Skip deploying production API
  --skip-test      # Skip testing the API
```

---

### 6. run_experiments_with_visualization.py

**Purpose:** Run Phoenix experiments with comprehensive visualization and quality evaluators

**Location:** `scripts/run_experiments_with_visualization.py` (425 lines)

**Experiment Configuration:**
```python
def get_experiment_configurations(args):
    # Get profiles from registry
    registry = get_registry()
    profiles = args.profiles if args.profiles else registry.list_profiles()

    # Strategy descriptions
    strategy_descriptions = {
        "binary_binary": "Binary",
        "float_float": "Float",
        "float_binary": "Float-Binary",
        "phased": "Phased",
        "hybrid_binary_bm25": "Hybrid + Desc",
        "bm25_only": "Text Only"
    }

    # Build configurations
    for profile in profiles:
        all_strategies = registry.list_ranking_strategies(profile)
        strategies_to_use = filter_strategies(all_strategies, args)

        configurations.append({
            "profile": profile,
            "strategies": [(s, strategy_descriptions[s]) for s in strategies_to_use]
        })

    return configurations
```

**Experiment Execution:**
```python
def main(args):
    # Initialize experiment runner
    with PhoenixExperimentRunner(
        experiment_project_name="experiments",
        enable_quality_evaluators=args.quality_evaluators,
        enable_llm_evaluators=args.llm_evaluators,
        evaluator_name=args.evaluator,
        llm_model=args.llm_model
    ) as runner:

        # Create/get dataset
        dataset = runner.create_experiment_dataset(
            dataset_name=args.dataset_name,
            csv_path=args.csv_path,
            force_new=args.force_new
        )

        # Run experiments for each profile/strategy
        for config in EXPERIMENT_CONFIGURATIONS:
            for strategy, description in config["strategies"]:
                result = runner.run_experiment(
                    profile=config["profile"],
                    strategy=strategy,
                    dataset=dataset,
                    description=f"{profile} - {description}"
                )
                all_experiments.append(result)
```

**Visualization Generation:**
```python
def create_visualization_tables(experiments):
    # 1. Profile summary
    profile_summary = [{
        "Profile": profile,
        "Total": stats["total"],
        "Success": stats["success"],
        "Failed": stats["failed"],
        "Success Rate": f"{stats['success']/stats['total']*100:.1f}%"
    } for profile, stats in profiles.items()]

    # 2. Detailed results
    detailed_results = [{
        "Profile": exp["profile"],
        "Strategy": exp["strategy"],
        "Status": "‚úÖ" if exp["status"] == "success" else "‚ùå",
        "Experiment Name": exp["experiment_name"]
    } for exp in experiments]

    # 3. Strategy comparison
    strategy_comparison = [...]  # Grouped by profile

    return {
        "profile_summary": pd.DataFrame(profile_summary),
        "detailed_results": pd.DataFrame(detailed_results),
        "strategy_comparison": pd.DataFrame(strategy_comparison)
    }
```

**Output:**
- Profile summary table
- Strategy comparison by profile
- Detailed experiment results
- CSV summary file
- JSON detailed results
- HTML integrated report (if quantitative tests exist)

**Command Line Arguments:**
```bash
python scripts/run_experiments_with_visualization.py \
  --dataset-name golden_eval_v1 \
  --csv-path data/testset/evaluation/video_search_queries.csv \
  --profiles frame_based_colpali \
  --quality-evaluators \
  --llm-evaluators \
  --evaluator visual_judge \
  --llm-model deepseek-r1:7b
```

---

### 7. manage_datasets.py

**Purpose:** CLI tool for managing evaluation datasets

**Location:** `scripts/manage_datasets.py` (58 lines)

**Operations:**

**List Datasets:**
```bash
python scripts/manage_datasets.py --list

# Output:
# Registered datasets:
#
# Name: golden_eval_v1
#   Phoenix ID: ds_abc123
#   Created: 2025-10-07 10:30:00
#   Examples: 50
#   Description: Golden evaluation dataset
```

**Create Dataset:**
```bash
python scripts/manage_datasets.py \
  --create my_dataset \
  --csv data/queries.csv

# Output:
# Dataset 'my_dataset' created with ID: ds_xyz789
```

**Get Dataset Info:**
```bash
python scripts/manage_datasets.py --info golden_eval_v1

# Output:
# Dataset: golden_eval_v1
#   phoenix_id: ds_abc123
#   created_at: 2025-10-07 10:30:00
#   num_examples: 50
#   description: Golden evaluation dataset
```

**Implementation:**
```python
def main():
    dm = DatasetManager()

    if args.list:
        datasets = dm.list_datasets()
        for ds in datasets:
            print(f"\nName: {ds['name']}")
            print(f"  Phoenix ID: {ds['phoenix_id']}")
            print(f"  Created: {ds['created_at']}")
            print(f"  Examples: {ds['num_examples']}")

    elif args.create and args.csv:
        dataset_id = dm.get_or_create_dataset(
            name=args.create,
            csv_path=args.csv,
            description=f"Created from {args.csv}"
        )

    elif args.info:
        info = dm.get_dataset_info(args.info)
        for key, value in info.items():
            print(f"  {key}: {value}")
```

---

### 8. phoenix_dashboard_standalone.py

**Purpose:** Interactive Streamlit dashboard for analytics, configuration, and system management

**Location:** `scripts/phoenix_dashboard_standalone.py` (200+ lines, multi-tab)

**Dashboard Tabs:**

**1. Analytics Tab** (PhoenixAnalytics):
```python
# Performance metrics over time
analytics = PhoenixAnalytics()
metrics = analytics.get_performance_metrics(time_range="7d")

# Display charts
st.plotly_chart(
    analytics.plot_latency_distribution(),
    use_container_width=True
)

st.plotly_chart(
    analytics.plot_error_rate_over_time(),
    use_container_width=True
)
```

**2. Evaluation Tab** (from phoenix_dashboard_evaluation_tab_tabbed):
```python
# Experiment comparison
render_evaluation_tab()
# - Experiment list
# - Side-by-side comparison
# - Metric visualization
# - Dataset management
```

**3. Config Management Tab** (from config_management_tab):
```python
# Tenant configuration
render_config_management_tab()
# - Create/update/delete configs
# - Profile selection
# - Strategy configuration
# - Schema management
```

**4. Memory Management Tab** (from memory_management_tab):
```python
# Conversation memory
render_memory_management_tab()
# - View memories by tenant
# - Search conversations
# - Memory analytics
# - Cache statistics
```

**5. Embedding Atlas Tab** (from embedding_atlas_tab):
```python
# Embedding visualization
render_embedding_atlas_tab()
# - 2D/3D embedding plots
# - Cluster analysis
# - Similarity search
# - Export embeddings
```

**6. Routing Evaluation Tab** (from routing_evaluation_tab):
```python
# Routing decision analysis
render_routing_evaluation_tab()
# - Routing accuracy metrics
# - Confusion matrix
# - Golden dataset comparison
# - Per-query analysis
```

**7. Orchestration Annotation Tab** (from orchestration_annotation_tab):
```python
# Multi-agent orchestration
render_orchestration_annotation_tab()
# - Workflow visualization
# - Agent communication logs
# - Dependency graphs
# - Performance bottlenecks
```

**Dashboard Features:**
- Auto-refresh capability
- Time range filtering
- Tenant isolation
- Export functionality
- Real-time metrics

**Startup:**
```bash
uv run streamlit run scripts/phoenix_dashboard_standalone.py --server.port 8501

# Then open: http://localhost:8501
```

---

## Data Flow

### 1. Video Ingestion Flow

```
User Command
    ‚îÇ
    ‚îú‚îÄ> run_ingestion.py
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Parse arguments
    ‚îÇ       ‚îÇ   ‚Ä¢ video_dir
    ‚îÇ       ‚îÇ   ‚Ä¢ profiles
    ‚îÇ       ‚îÇ   ‚Ä¢ backend
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Get profiles (from config or args)
    ‚îÇ       ‚îÇ   default: video_colpali_smol500_mv_frame
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ> FOR each profile:
    ‚îÇ               ‚îÇ
    ‚îÇ               ‚îú‚îÄ> Build Pipeline
    ‚îÇ               ‚îÇ   ‚Ä¢ Test mode ‚Üí build_test_pipeline()
    ‚îÇ               ‚îÇ   ‚Ä¢ Simple mode ‚Üí build_simple_pipeline()
    ‚îÇ               ‚îÇ   ‚Ä¢ Advanced mode ‚Üí create_pipeline().with_*().build()
    ‚îÇ               ‚îÇ
    ‚îÇ               ‚îú‚îÄ> Discover Videos
    ‚îÇ               ‚îÇ   ‚Ä¢ video_dir.glob('*.mp4')
    ‚îÇ               ‚îÇ
    ‚îÇ               ‚îú‚îÄ> Process Concurrently
    ‚îÇ               ‚îÇ   ‚Ä¢ max_concurrent=3 (default)
    ‚îÇ               ‚îÇ   ‚Ä¢ async video processing
    ‚îÇ               ‚îÇ   ‚îÇ
    ‚îÇ               ‚îÇ   ‚îî‚îÄ> FOR each video:
    ‚îÇ               ‚îÇ           ‚îú‚îÄ> Extract frames/chunks
    ‚îÇ               ‚îÇ           ‚îú‚îÄ> Generate embeddings
    ‚îÇ               ‚îÇ           ‚îú‚îÄ> Build documents
    ‚îÇ               ‚îÇ           ‚îî‚îÄ> Upload to Vespa
    ‚îÇ               ‚îÇ
    ‚îÇ               ‚îî‚îÄ> Collect Results
    ‚îÇ                   ‚Ä¢ successful videos
    ‚îÇ                   ‚Ä¢ documents fed
    ‚îÇ                   ‚Ä¢ processing time
    ‚îÇ                   ‚Ä¢ throughput
    ‚îÇ
    ‚îî‚îÄ> Print Summary
        ‚Ä¢ Per-profile statistics
        ‚Ä¢ Overall success rate
        ‚Ä¢ Total documents processed
```

### 2. Schema Deployment Flow

```
User Command
    ‚îÇ
    ‚îú‚îÄ> deploy_json_schema.py OR deploy_all_schemas.py
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Load Schema Files
    ‚îÇ       ‚îÇ   ‚Ä¢ Single: specified file
    ‚îÇ       ‚îÇ   ‚Ä¢ All: configs/schemas/*.json
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Parse Schemas
    ‚îÇ       ‚îÇ   ‚Ä¢ JsonSchemaParser.parse_schema()
    ‚îÇ       ‚îÇ   ‚Ä¢ Validate structure
    ‚îÇ       ‚îÇ   ‚Ä¢ Convert to Vespa Schema objects
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Create Application Package
    ‚îÇ       ‚îÇ   ‚Ä¢ ApplicationPackage(name="videosearch")
    ‚îÇ       ‚îÇ   ‚Ä¢ Add schema(s)
    ‚îÇ       ‚îÇ   ‚Ä¢ Add validation overrides (if multi-schema)
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Generate ZIP
    ‚îÇ       ‚îÇ   ‚Ä¢ app_package.to_zip()
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Deploy via HTTP
    ‚îÇ       ‚îÇ   POST http://localhost:19071/application/v2/tenant/default/prepareandactivate
    ‚îÇ       ‚îÇ   ‚Ä¢ Content-Type: application/zip
    ‚îÇ       ‚îÇ   ‚Ä¢ Body: ZIP bytes
    ‚îÇ       ‚îÇ   ‚Ä¢ Timeout: 60s
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Wait for Propagation
    ‚îÇ       ‚îÇ   ‚Ä¢ sleep(5)
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Verify Deployment
    ‚îÇ       ‚îÇ   GET http://localhost:8080/ApplicationStatus
    ‚îÇ       ‚îÇ   ‚Ä¢ Check 200 OK
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ> Extract Ranking Strategies (if deploy_all_schemas)
    ‚îÇ           ‚Ä¢ Parse rank-profiles from schemas
    ‚îÇ           ‚Ä¢ Save to ranking_strategies.json
    ‚îÇ
    ‚îî‚îÄ> Print Success/Failure
```

### 3. Experiment Workflow Flow

```
User Command
    ‚îÇ
    ‚îú‚îÄ> run_experiments_with_visualization.py
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Initialize PhoenixExperimentRunner
    ‚îÇ       ‚îÇ   ‚Ä¢ Separate "experiments" project
    ‚îÇ       ‚îÇ   ‚Ä¢ Quality evaluators: relevance, diversity, distribution
    ‚îÇ       ‚îÇ   ‚Ä¢ LLM evaluators: reference-free, reference-based
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Prepare Dataset
    ‚îÇ       ‚îÇ   IF --list-datasets:
    ‚îÇ       ‚îÇ       ‚Ä¢ List all registered datasets
    ‚îÇ       ‚îÇ       ‚Ä¢ Exit
    ‚îÇ       ‚îÇ   ELSE:
    ‚îÇ       ‚îÇ       ‚Ä¢ Load or create dataset from CSV
    ‚îÇ       ‚îÇ       ‚Ä¢ Register with Phoenix
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Get Experiment Configurations
    ‚îÇ       ‚îÇ   ‚Ä¢ Query strategy registry
    ‚îÇ       ‚îÇ   ‚Ä¢ Filter profiles (--profiles or all)
    ‚îÇ       ‚îÇ   ‚Ä¢ Filter strategies (--strategies or common)
    ‚îÇ       ‚îÇ   ‚Ä¢ Build profile √ó strategy matrix
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Run Experiments
    ‚îÇ       ‚îÇ   FOR each profile:
    ‚îÇ       ‚îÇ       FOR each strategy:
    ‚îÇ       ‚îÇ           ‚îÇ
    ‚îÇ       ‚îÇ           ‚îú‚îÄ> Create Experiment
    ‚îÇ       ‚îÇ           ‚îÇ   ‚Ä¢ Name: "{profile} - {strategy}"
    ‚îÇ       ‚îÇ           ‚îÇ   ‚Ä¢ Attach to dataset
    ‚îÇ       ‚îÇ           ‚îÇ
    ‚îÇ       ‚îÇ           ‚îú‚îÄ> Run Search Queries
    ‚îÇ       ‚îÇ           ‚îÇ   FOR each query in dataset:
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Execute search
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Record spans
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Collect results
    ‚îÇ       ‚îÇ           ‚îÇ
    ‚îÇ       ‚îÇ           ‚îú‚îÄ> Evaluate Results
    ‚îÇ       ‚îÇ           ‚îÇ   IF quality_evaluators:
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Relevance score
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Diversity score
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Distribution metrics
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Temporal coverage
    ‚îÇ       ‚îÇ           ‚îÇ   IF llm_evaluators:
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Reference-free evaluation
    ‚îÇ       ‚îÇ           ‚îÇ       ‚Ä¢ Reference-based comparison
    ‚îÇ       ‚îÇ           ‚îÇ
    ‚îÇ       ‚îÇ           ‚îî‚îÄ> Store Results
    ‚îÇ       ‚îÇ               ‚Ä¢ Experiment ID
    ‚îÇ       ‚îÇ               ‚Ä¢ Status (success/failed)
    ‚îÇ       ‚îÇ               ‚Ä¢ Evaluation scores
    ‚îÇ       ‚îÇ               ‚Ä¢ Span traces
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Generate Visualizations
    ‚îÇ       ‚îÇ   ‚Ä¢ Profile summary table
    ‚îÇ       ‚îÇ   ‚Ä¢ Strategy comparison (grouped by profile)
    ‚îÇ       ‚îÇ   ‚Ä¢ Detailed results (all experiments)
    ‚îÇ       ‚îÇ   ‚Ä¢ Print to console with tabulate
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Export Results
    ‚îÇ       ‚îÇ   ‚Ä¢ CSV: outputs/experiment_results/experiment_summary_*.csv
    ‚îÇ       ‚îÇ   ‚Ä¢ JSON: outputs/experiment_results/experiment_details_*.json
    ‚îÇ       ‚îÇ   ‚Ä¢ HTML: generate_integrated_report() if quantitative tests exist
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ> Print Phoenix UI Links
    ‚îÇ           ‚Ä¢ Dataset URL: http://localhost:6006/datasets/{id}
    ‚îÇ           ‚Ä¢ Experiments Project: http://localhost:6006/projects/experiments
    ‚îÇ           ‚Ä¢ Default Project: http://localhost:6006/projects/default
    ‚îÇ
    ‚îî‚îÄ> Exit with Summary Statistics
```

### 4. Optimization & Deployment Flow

```
User Command
    ‚îÇ
    ‚îú‚îÄ> run_optimization.py
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Step 1: Run Orchestrator
    ‚îÇ       ‚îÇ   ‚Ä¢ Execute: python -m src.optimizer.orchestrator
    ‚îÇ       ‚îÇ   ‚Ä¢ Timeout: 2 hours
    ‚îÇ       ‚îÇ   ‚Ä¢ Output: optimization_results/unified_router_prompt_artifact.json
    ‚îÇ       ‚îÇ   ‚Ä¢ Contains: optimized prompts, chain-of-thought examples
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Step 2: Upload Artifacts to Modal
    ‚îÇ       ‚îÇ   IF --skip-upload:
    ‚îÇ       ‚îÇ       ‚Ä¢ Skip this step
    ‚îÇ       ‚îÇ   ELSE:
    ‚îÇ       ‚îÇ       ‚Ä¢ modal volume put optimization-artifacts
    ‚îÇ       ‚îÇ       ‚Ä¢ Target: /artifacts/unified_router_prompt_artifact.json
    ‚îÇ       ‚îÇ       ‚Ä¢ Timeout: 5 minutes
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Step 3: Deploy Production API
    ‚îÇ       ‚îÇ   IF --skip-deploy:
    ‚îÇ       ‚îÇ       ‚Ä¢ Skip this step
    ‚îÇ       ‚îÇ   ELSE:
    ‚îÇ       ‚îÇ       ‚îú‚îÄ> Check HuggingFace Secret
    ‚îÇ       ‚îÇ       ‚îÇ   ‚Ä¢ modal secret list
    ‚îÇ       ‚îÇ       ‚îÇ   IF not exists:
    ‚îÇ       ‚îÇ       ‚îÇ       ‚Ä¢ Get HF_TOKEN from environment
    ‚îÇ       ‚îÇ       ‚îÇ       ‚Ä¢ modal secret create huggingface-token HF_TOKEN=...
    ‚îÇ       ‚îÇ       ‚îÇ
    ‚îÇ       ‚îÇ       ‚îî‚îÄ> Deploy to Modal
    ‚îÇ       ‚îÇ           ‚Ä¢ modal deploy src/inference/modal_inference_service.py
    ‚îÇ       ‚îÇ           ‚Ä¢ Timeout: 10 minutes
    ‚îÇ       ‚îÇ           ‚Ä¢ Return: API URL
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> Step 4: Test Production API
    ‚îÇ       ‚îÇ   IF --skip-test:
    ‚îÇ       ‚îÇ       ‚Ä¢ Skip this step
    ‚îÇ       ‚îÇ   ELSE:
    ‚îÇ       ‚îÇ       FOR each test_case:
    ‚îÇ       ‚îÇ           ‚Ä¢ POST {api_url} with query
    ‚îÇ       ‚îÇ           ‚Ä¢ Verify: search_modality, generation_type
    ‚îÇ       ‚îÇ           ‚Ä¢ Check: latency, HTTP status
    ‚îÇ       ‚îÇ           ‚Ä¢ Print: ‚úÖ or ‚ùå
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ> Print Summary
    ‚îÇ           ‚Ä¢ Total time
    ‚îÇ           ‚Ä¢ Artifacts path
    ‚îÇ           ‚Ä¢ API URL
    ‚îÇ           ‚Ä¢ Test results
    ‚îÇ           ‚Ä¢ Usage example (curl command)
    ‚îÇ
    ‚îî‚îÄ> Exit with Status Code
```

---

## Usage Examples

### Example 1: Basic Video Ingestion

```bash
# Process videos with default profile
JAX_PLATFORM_NAME=cpu uv run python scripts/run_ingestion.py \
  --video_dir data/testset/evaluation/sample_videos \
  --backend vespa

# Output:
# ============================================================
# üéØ Processing with profile: video_colpali_smol500_mv_frame
# ============================================================
# üé¨ Starting Video Processing Pipeline
# üìÅ Video directory: data/testset/evaluation/sample_videos
# üìÇ Output directory: outputs/ingestion/video_colpali_smol500_mv_frame
# üîß Backend: vespa
# üìπ Found 3 videos to process
#
# ‚úÖ Profile video_colpali_smol500_mv_frame completed!
#    Time: 45.32 seconds
#    Videos: 3/3 successful
#    Documents fed: 180
#    Throughput: 4.0 docs/sec
#    Avg per video: 15.11 seconds
```

### Example 2: Multi-Profile Ingestion

```bash
# Process with multiple profiles simultaneously
JAX_PLATFORM_NAME=cpu uv run python scripts/run_ingestion.py \
  --video_dir data/testset/evaluation/sample_videos \
  --backend vespa \
  --profile video_colpali_smol500_mv_frame \
           video_videoprism_base_mv_chunk_30s \
           video_colqwen_omni_mv_chunk_30s

# Output shows processing for each profile:
# ============================================================
# üéØ Processing with profile: video_colpali_smol500_mv_frame
# ============================================================
# ...
# ‚úÖ Profile video_colpali_smol500_mv_frame completed!
#
# ============================================================
# üéØ Processing with profile: video_videoprism_base_mv_chunk_30s
# ============================================================
# ...
# ‚úÖ Profile video_videoprism_base_mv_chunk_30s completed!
#
# ============================================================
# üìä Overall Summary
# ============================================================
# Processed 3 profiles
# ‚úÖ video_colpali_smol500_mv_frame: 3/3 videos succeeded, 180 docs in 45.3s
# ‚úÖ video_videoprism_base_mv_chunk_30s: 3/3 videos succeeded, 90 docs in 38.1s
# ‚ö†Ô∏è video_colqwen_omni_mv_chunk_30s: 2/3 videos succeeded, 120 docs in 52.7s
```

### Example 3: Test Mode Ingestion

```bash
# Quick validation with limited frames
uv run python scripts/run_ingestion.py \
  --video_dir data/testset/evaluation/sample_videos \
  --backend vespa \
  --test-mode \
  --max-frames 10

# Output:
# üß™ Using test pipeline builder...
# üñºÔ∏è Max frames: 10
# ‚úÖ Profile video_colpali_smol500_mv_frame completed!
#    Time: 12.45 seconds
#    Videos: 3/3 successful
#    Documents fed: 30  # Only 10 frames per video
```

### Example 4: Schema Deployment

```bash
# Deploy single schema
python scripts/deploy_json_schema.py \
  configs/schemas/video_colpali_smol500_mv_frame.json

# Output:
# ============================================================
# Vespa JSON Schema Deployment
# ============================================================
# Schema file: configs/schemas/video_colpali_smol500_mv_frame.json
# Config server: localhost:19071
# Data endpoint: localhost:8080
#
# üìÑ Loading schema from video_colpali_smol500_mv_frame.json
# üì¶ Processing schema: video_colpali_smol500_mv_frame
# üöÄ Deploying to http://localhost:19071/application/v2/tenant/default/prepareandactivate...
# ‚úÖ Schema 'video_colpali_smol500_mv_frame' deployed successfully!
#
# ‚è≥ Waiting for deployment to propagate...
#
# üîç Verifying 'video_colpali_smol500_mv_frame' deployment...
# ‚úÖ Vespa is running and responding
#
# ============================================================
# Deployment complete!
# ============================================================

# Deploy all schemas at once
python scripts/deploy_all_schemas.py

# Output:
# üöÄ Found 8 schemas to deploy
# üìÑ Loading schema from video_colpali_smol500_mv_frame.json
# ‚úÖ Added schema: video_colpali_smol500_mv_frame
# üìÑ Loading schema from video_videoprism_base_mv_chunk_30s.json
# ‚úÖ Added schema: video_videoprism_base_mv_chunk_30s
# ...
# üì¶ Deploying all schemas to Vespa...
# ‚úÖ All schemas deployed successfully!
# üìä Extracting ranking strategies from all schemas...
# ‚úÖ Extracted 42 ranking strategies from 8 schemas
# üéâ Schema deployment complete!
```

### Example 5: Phoenix Experiments

```bash
# Run experiments with quality evaluators
uv run python scripts/run_experiments_with_visualization.py \
  --dataset-name golden_eval_v1 \
  --csv-path data/testset/evaluation/video_search_queries.csv \
  --profiles frame_based_colpali \
  --quality-evaluators

# Output:
# ================================================================================
# PHOENIX EXPERIMENTS WITH VISUALIZATION
# ================================================================================
#
# Timestamp: 2025-10-07 14:30:00
# Experiment Project: experiments (separate from default traces)
# Quality Evaluators: ‚úÖ ENABLED (relevance, diversity, distribution, temporal coverage)
# LLM Evaluators: ‚ùå DISABLED
#
# Preparing experiment dataset...
# ‚úÖ Dataset ready: http://localhost:6006/datasets/ds_abc123
#
# ============================================================
# Profile: frame_based_colpali
# ============================================================
#
# [1/6] Frame Based Colpali - Binary
#   Strategy: binary_binary
#   ‚úÖ Success
#
# [2/6] Frame Based Colpali - Float
#   Strategy: float_float
#   ‚úÖ Success
#
# [3/6] Frame Based Colpali - Phased
#   Strategy: phased
#   ‚úÖ Success
#
# ...
#
# ================================================================================
# EXPERIMENT RESULTS VISUALIZATION
# ================================================================================
#
# üìä PROFILE SUMMARY
# ------------------------------------------------------------
# +---------------------+-------+---------+--------+---------------+
# | Profile             | Total | Success | Failed | Success Rate  |
# +=====================+=======+=========+========+===============+
# | frame_based_colpali |     6 |       6 |      0 | 100.0%        |
# +---------------------+-------+---------+--------+---------------+
#
# üîç STRATEGY COMPARISON BY PROFILE
# ------------------------------------------------------------
#
# frame_based_colpali:
# Strategy              Description         Status
# --------------------  ------------------  -------------
# binary_binary         Binary              ‚úÖ Success
# float_float           Float               ‚úÖ Success
# float_binary          Float-Binary        ‚úÖ Success
# phased                Phased              ‚úÖ Success
# hybrid_binary_bm25    Hybrid + Desc       ‚úÖ Success
# bm25_only             Text Only           ‚úÖ Success
#
# ================================================================================
# SUMMARY STATISTICS
# ================================================================================
#
# Total Experiments Attempted: 6
# Successful: 6 (100.0%)
# Failed: 0 (0.0%)
#
# ================================================================================
# VIEW IN PHOENIX UI
# ================================================================================
#
# üîó Dataset: http://localhost:6006/datasets/ds_abc123
# üîó Experiments Project: http://localhost:6006/projects/experiments
# üîó Default Project (spans): http://localhost:6006/projects/default
#
# ‚ÑπÔ∏è  Notes:
#   - Experiments are in separate 'experiments' project
#   - Each experiment has its own traces with detailed spans
#   - Use Phoenix UI to compare experiments side-by-side
#   - Evaluation scores are attached to each experiment
#
# üíæ Results saved to: outputs/experiment_results/experiment_summary_20251007_143000.csv
# üíæ Detailed results saved to: outputs/experiment_results/experiment_details_20251007_143000.json
#
# ‚úÖ All experiments completed!
```

### Example 6: Complete Optimization Workflow

```bash
# Run full optimization and deployment
python scripts/run_optimization.py --config config.json

# Output:
# üéØ Agentic Router Complete Optimization & Deployment
# ============================================================
#
# üöÄ Starting Orchestrator Optimization...
# ============================================================
# [Orchestrator runs for ~30-60 minutes...]
# ‚úÖ Orchestrator completed successfully
# üìÑ Artifacts found: optimization_results/unified_router_prompt_artifact.json
#
# üì§ Uploading artifacts to Modal volume...
# ‚úÖ Artifacts uploaded to Modal volume
#
# üöÄ Deploying Production API...
# ‚ö†Ô∏è  Modal secret 'huggingface-token' not found
# üìù Creating Modal secret...
# ‚úÖ Modal secret created successfully
# ‚úÖ Production API deployed successfully
# üåê API URL: https://agentic-router-production-route.modal.run
#
# üß™ Testing Production API...
#   Test 1: 'Show me how to cook pasta'
#     ‚úÖ video/raw_results (125.3ms)
#   Test 2: 'Create a detailed report on climate change'
#     ‚úÖ text/detailed_report (98.7ms)
#   Test 3: 'What's the summary of the AI paper?'
#     ‚úÖ text/summary (87.2ms)
# ‚úÖ All API tests completed
#
# ============================================================
# üéâ DEPLOYMENT COMPLETE!
# ============================================================
# ‚è±Ô∏è  Total time: 45.3 minutes
# üìÑ Artifacts: optimization_results/unified_router_prompt_artifact.json
# üåê API URL: https://agentic-router-production-route.modal.run
# ‚úÖ Tests: Passed
#
# üîó Usage:
# curl -X POST https://agentic-router-production-route.modal.run \
#   -H 'Content-Type: application/json' \
#   -d '{"user_query": "Show me cooking videos"}'
```

### Example 7: Dataset Management

```bash
# List all datasets
python scripts/manage_datasets.py --list

# Output:
# Registered datasets:
#
# Name: golden_eval_v1
#   Phoenix ID: ds_abc123
#   Created: 2025-10-05 10:30:00
#   Examples: 50
#   Description: Golden evaluation dataset v1
#
# Name: video_search_test
#   Phoenix ID: ds_def456
#   Created: 2025-10-06 14:15:00
#   Examples: 25
#   Description: Test queries for video search

# Create new dataset
python scripts/manage_datasets.py \
  --create my_queries \
  --csv data/my_queries.csv

# Output:
# Dataset 'my_queries' created with ID: ds_ghi789

# Get dataset info
python scripts/manage_datasets.py --info golden_eval_v1

# Output:
# Dataset: golden_eval_v1
#   phoenix_id: ds_abc123
#   created_at: 2025-10-05 10:30:00
#   num_examples: 50
#   description: Golden evaluation dataset v1
```

### Example 8: Interactive Dashboard

```bash
# Start Phoenix dashboard
uv run streamlit run scripts/phoenix_dashboard_standalone.py --server.port 8501

# Output:
# You can now view your Streamlit app in your browser.
#
#   Local URL: http://localhost:8501
#   Network URL: http://192.168.1.100:8501
#
# Dashboard features:
# - Analytics: Performance metrics, latency distribution, error rates
# - Evaluation: Experiment comparison, metric visualization
# - Config Management: Tenant configuration, profile selection
# - Memory Management: Conversation history, cache stats
# - Embedding Atlas: 2D/3D visualization, cluster analysis
# - Routing Evaluation: Routing accuracy, confusion matrix
# - Orchestration: Multi-agent workflow visualization

# Access in browser: http://localhost:8501
# Select tab from sidebar to explore different features
```

---

## Production Considerations

### 1. Performance Optimization

**Ingestion Throughput:**
```python
# Adjust concurrency based on available resources
--max-concurrent 5  # More concurrent videos (CPU/memory intensive)
--max-concurrent 1  # Serial processing (safer for limited resources)

# Throughput metrics:
# - ColPali frame-based: ~4-5 docs/sec (GPU)
# - VideoPrism global: ~2-3 docs/sec (GPU)
# - ColQwen chunk-based: ~3-4 docs/sec (GPU)
```

**Async Processing:**
```python
# Use async methods for I/O-bound operations
results = await pipeline.process_videos_concurrent(
    video_files,
    max_concurrent=3
)

# Benefits:
# - Non-blocking video processing
# - Better CPU utilization
# - Faster overall throughput
```

**Schema Deployment:**
```python
# Deploy all schemas at once (faster than sequential)
python scripts/deploy_all_schemas.py  # Single deployment

# vs

# Multiple sequential deployments (slower)
for schema in schemas:
    python scripts/deploy_json_schema.py {schema}
```

### 2. Error Handling

**Ingestion Errors:**
```python
# Pipeline continues on individual video failures
result = {
    'status': 'failed',
    'error': 'Embedding generation failed',
    'video_path': str(video_path)
}

# Overall statistics still reported:
# ‚ö†Ô∏è Profile frame_based_colpali partially completed!
#    Time: 52.7 seconds
#    Videos: 2/3 successful
#    Failed: 1 videos
```

**Schema Deployment Errors:**
```python
# Common errors:
# - "Connection refused" ‚Üí Vespa not running
# - "HTTP 400" ‚Üí Schema validation failed
# - "Timeout" ‚Üí Vespa busy or unresponsive

# Verification step catches deployment failures:
if not verify_deployment(schema_name):
    logger.error("Deployment verification failed")
    sys.exit(1)
```

**Experiment Errors:**
```python
# Experiments continue on individual strategy failures
if result["status"] == "failed":
    error = result.get("error", "Unknown error")
    if "Text encoder not available" in error:
        print(f"  ‚ö†Ô∏è  Skipped: Encoder not available")
    else:
        print(f"  ‚ùå Failed: {error[:50]}...")

# Final summary shows partial success:
# Total: 10, Successful: 8 (80.0%), Failed: 2 (20.0%)
```

### 3. Monitoring and Logging

**Ingestion Logs:**
```python
# Logs written to outputs/logs/
# - ingestion_pipeline.log  # Main pipeline logs
# - video_processing.log    # Per-video processing
# - embedding_generation.log  # Embedding logs
# - vespa_upload.log        # Upload logs

# View logs:
tail -f outputs/logs/ingestion_pipeline.log
```

**Experiment Logs:**
```python
# Experiment results saved to:
# - outputs/experiment_results/experiment_summary_*.csv
# - outputs/experiment_results/experiment_details_*.json

# Phoenix spans capture:
# - Query execution time
# - Search latency
# - Embedding generation time
# - Evaluation scores

# View in Phoenix UI: http://localhost:6006
```

**Dashboard Monitoring:**
```python
# Real-time metrics in Streamlit dashboard:
# - Request latency (p50, p95, p99)
# - Error rate over time
# - Cache hit rate
# - Throughput (requests/sec)

# Auto-refresh every 30s (configurable)
st.session_state.auto_refresh = True
```

### 4. Resource Management

**Memory Management:**
```python
# Video processing is memory-intensive
# Estimated memory per video:
# - Frame extraction: ~200-500 MB
# - Embedding generation: ~1-2 GB (GPU)
# - Document building: ~50-100 MB

# Total concurrent memory:
# max_concurrent=3 ‚Üí ~4-6 GB peak memory

# Recommendations:
# - 16 GB RAM minimum for production
# - 32 GB RAM recommended for max_concurrent > 5
```

**GPU Utilization:**
```python
# GPU required for:
# - ColPali embedding generation
# - VideoPrism encoding
# - ColQwen embedding generation

# GPU memory requirements:
# - ColPali Smol 500M: ~2 GB VRAM
# - VideoPrism Base: ~4 GB VRAM
# - ColQwen Omni: ~6 GB VRAM

# For multiple profiles:
# - Process sequentially (GPU memory limits)
# - Or use multiple GPUs with CUDA_VISIBLE_DEVICES
```

**Disk Space:**
```python
# Storage requirements per video:
# - Original video: ~50-500 MB
# - Extracted frames: ~10-50 MB
# - Transcripts: ~10-50 KB
# - Embeddings (binary): ~100-500 KB
# - Embeddings (float): ~1-5 MB

# Total for 1000 videos: ~100-500 GB
```

### 5. Scaling Strategies

**Horizontal Scaling:**
```python
# Ingestion:
# - Run multiple ingestion processes
# - Partition videos by directory
# - Each process handles different profiles

# Example:
# Process 1: --profile video_colpali_smol500_mv_frame
# Process 2: --profile video_videoprism_base_mv_chunk_30s
# Process 3: --profile video_colqwen_omni_mv_chunk_30s
```

**Vertical Scaling:**
```python
# Increase concurrency with more resources:
--max-concurrent 10  # Requires 32+ GB RAM, 16+ CPU cores

# Benefits:
# - Faster overall throughput
# - Better resource utilization
# - Reduced total processing time
```

**Batch Processing:**
```python
# Process large video collections in batches:
videos_per_batch = 100
for i in range(0, len(videos), videos_per_batch):
    batch = videos[i:i+videos_per_batch]
    run_ingestion(batch)
```

### 6. Best Practices

**Schema Management:**
```python
# 1. Always use deploy_all_schemas.py for consistency
# 2. Version control all schema JSON files
# 3. Test schema changes in staging before production
# 4. Extract ranking strategies after deployment
# 5. Validate schemas before deployment
```

**Ingestion Pipeline:**
```python
# 1. Test with --test-mode first (max_frames=10)
# 2. Process small batches before full ingestion
# 3. Monitor logs for errors during processing
# 4. Verify documents in Vespa after ingestion
# 5. Use appropriate backend (vespa for production)
```

**Experiments:**
```python
# 1. Use separate "experiments" project in Phoenix
# 2. Enable quality evaluators for comprehensive metrics
# 3. Save results to CSV/JSON for analysis
# 4. Compare experiments side-by-side in Phoenix UI
# 5. Export results before deleting experiments
```

**Dashboard:**
```python
# 1. Use dedicated server for production dashboard
# 2. Enable authentication for multi-tenant access
# 3. Set appropriate auto-refresh intervals
# 4. Monitor resource usage (CPU, memory)
# 5. Export metrics regularly for long-term analysis
```

### 7. Common Issues and Solutions

**Issue: "Video processing failed: CUDA out of memory"**
```bash
# Solution: Reduce concurrency or batch size
--max-concurrent 1  # Process one video at a time
--max-frames 50     # Reduce frames per video
```

**Issue: "Schema deployment failed: Connection refused"**
```bash
# Solution: Ensure Vespa is running
docker ps | grep vespa  # Check Vespa container
docker-compose up vespa  # Start Vespa if not running
```

**Issue: "Experiment failed: Text encoder not available"**
```bash
# Solution: Ensure required models are downloaded
python scripts/setup_video_processing.py  # Download models
```

**Issue: "Dashboard not loading: ModuleNotFoundError"**
```bash
# Solution: Install all dependencies
uv pip install -r requirements.txt
uv pip install streamlit plotly tabulate
```

---

## Summary

The Scripts & Operations module provides comprehensive tooling for:

1. **Video Ingestion**: Builder pattern pipeline with multi-profile support
2. **Schema Management**: JSON-based deployment with validation
3. **Optimization**: Complete DSPy optimization and deployment workflow
4. **Experimentation**: Phoenix experiments with quality evaluators
5. **Dataset Management**: CRUD operations for evaluation datasets
6. **Interactive Dashboard**: Streamlit-based analytics and management UI
7. **System Setup**: Environment initialization and dependency checking

**Key Design Patterns:**
- Builder pattern for flexible pipeline configuration
- Async processing for concurrent video handling
- Command pattern for CLI tool design
- Context manager for experiment runner lifecycle
- Factory pattern for strategy resolution

**Production Features:**
- Concurrent async video processing
- Multi-profile simultaneous ingestion
- Phoenix experiment tracking with visualization
- Comprehensive error handling and logging
- Resource-aware concurrency limits
- Interactive Streamlit dashboards

This module serves as the operational backbone of the Cogniverse system, providing production-grade tools for deployment, ingestion, optimization, and monitoring.

---

**Next Study Guides:**
- **15_UI_DASHBOARD.md**: Detailed Streamlit dashboard components
- **16_SYSTEM_INTEGRATION.md**: End-to-end system integration
- **17_INSTRUMENTATION.md**: Phoenix telemetry and observability
