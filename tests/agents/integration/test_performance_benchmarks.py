"""
Performance Benchmarking Tests

Tests performance characteristics of the multi-agent system:
1. Response time benchmarks for different query types
2. Memory usage profiling during sustained operations
3. Concurrent request handling capacity
4. Component initialization times
5. End-to-end pipeline performance
"""

import asyncio
import os
import time
from unittest.mock import patch

import psutil
import pytest

from src.app.agents.detailed_report_agent import DetailedReportAgent
from src.app.agents.enhanced_routing_agent import EnhancedRoutingAgent
from src.app.agents.summarizer_agent import SummarizerAgent
from src.app.routing.adaptive_threshold_learner import AdaptiveThresholdLearner
from src.app.routing.query_enhancement_engine import QueryEnhancementPipeline
from src.app.routing.relationship_extraction_tools import RelationshipExtractorTool


@pytest.mark.integration
class TestPerformanceBenchmarks:
    """Performance benchmarking for multi-agent system components"""

    @pytest.fixture(autouse=True)
    def setup_environment(self):
        """Set up environment for performance testing"""
        os.environ["VESPA_SCHEMA"] = "video_colpali_smol500_mv_frame"
        yield

    def test_component_initialization_times(self):
        """Benchmark initialization times for all components"""
        components_to_test = [
            ("RelationshipExtractorTool", RelationshipExtractorTool),
            ("QueryEnhancementPipeline", QueryEnhancementPipeline),
            ("SummarizerAgent", SummarizerAgent),
            ("DetailedReportAgent", DetailedReportAgent),
            ("AdaptiveThresholdLearner", AdaptiveThresholdLearner),
        ]

        initialization_times = {}

        for component_name, component_class in components_to_test:
            start_time = time.time()
            try:
                component = component_class()
                end_time = time.time()
                initialization_time = end_time - start_time
                initialization_times[component_name] = initialization_time

                print(f"✅ {component_name}: {initialization_time:.3f}s")

                # Verify component is functional
                assert component is not None

            except Exception as e:
                end_time = time.time()
                initialization_time = end_time - start_time
                print(
                    f"⚠️  {component_name}: {initialization_time:.3f}s (with error: {e})"
                )
                initialization_times[component_name] = initialization_time

        # Performance assertions
        for component_name, init_time in initialization_times.items():
            # Components should initialize within reasonable time
            assert (
                init_time < 30.0
            ), f"{component_name} took too long to initialize: {init_time:.3f}s"

        avg_init_time = sum(initialization_times.values()) / len(initialization_times)
        print(f"Average initialization time: {avg_init_time:.3f}s")

    def test_routing_agent_performance(self):
        """Benchmark routing agent response times"""
        with patch("src.common.config.get_config") as mock_config:
            mock_config.return_value = {
                "video_agent_url": "http://localhost:8002",
                "summarizer_agent_url": "http://localhost:8003",
                "detailed_report_agent_url": "http://localhost:8004",
            }

            routing_agent = EnhancedRoutingAgent()

            test_queries = [
                "Find videos of robots",
                "Summarize AI research",
                "Generate detailed report on machine learning",
                "Search for autonomous vehicle videos",
                "Analyze computer vision papers",
            ]

            response_times = []

            for query in test_queries:
                start_time = time.time()
                try:
                    asyncio.run(routing_agent.route_query(query))
                    end_time = time.time()
                    response_time = end_time - start_time
                    response_times.append(response_time)

                    print(f"Query '{query[:30]}...': {response_time:.3f}s")

                except Exception:
                    end_time = time.time()
                    response_time = end_time - start_time
                    response_times.append(response_time)
                    print(f"Query handled gracefully: {response_time:.3f}s")

            if response_times:
                avg_response_time = sum(response_times) / len(response_times)
                max_response_time = max(response_times)
                min_response_time = min(response_times)

                print(
                    f"Routing performance - Avg: {avg_response_time:.3f}s, Max: {max_response_time:.3f}s, Min: {min_response_time:.3f}s"
                )

                # Performance assertions
                assert (
                    avg_response_time < 10.0
                ), f"Average response time too high: {avg_response_time:.3f}s"
                assert (
                    max_response_time < 20.0
                ), f"Max response time too high: {max_response_time:.3f}s"

    def test_relationship_extraction_performance(self):
        """Benchmark relationship extraction performance"""
        extractor = RelationshipExtractorTool()

        test_queries = [
            "Find videos of robots playing soccer",
            "Show me research about machine learning in autonomous vehicles",
            "Compare different AI approaches to computer vision problems",
            "Analyze the relationship between deep learning and robotics",
            "Search for videos demonstrating reinforcement learning applications",
        ]

        extraction_times = []

        for query in test_queries:
            start_time = time.time()
            try:
                result = asyncio.run(
                    extractor.extract_comprehensive_relationships(query)
                )
                end_time = time.time()
                extraction_time = end_time - start_time
                extraction_times.append(extraction_time)

                entities = result.get("entities", [])
                relationships = result.get("relationships", [])
                print(
                    f"Query '{query[:30]}...': {extraction_time:.3f}s ({len(entities)} entities, {len(relationships)} relationships)"
                )

            except Exception:
                end_time = time.time()
                extraction_time = end_time - start_time
                extraction_times.append(extraction_time)
                print(f"Extraction handled gracefully: {extraction_time:.3f}s")

        if extraction_times:
            avg_extraction_time = sum(extraction_times) / len(extraction_times)
            print(f"Average extraction time: {avg_extraction_time:.3f}s")

            # Performance assertion
            assert (
                avg_extraction_time < 15.0
            ), f"Average extraction time too high: {avg_extraction_time:.3f}s"

    def test_query_enhancement_performance(self):
        """Benchmark query enhancement pipeline performance"""
        pipeline = QueryEnhancementPipeline()

        test_cases = [
            {
                "query": "Find videos of autonomous robots",
                "entities": [
                    {"text": "robots", "label": "ENTITY"},
                    {"text": "autonomous", "label": "MODIFIER"},
                ],
                "relationships": [
                    {"subject": "robots", "relation": "type", "object": "autonomous"}
                ],
            },
            {
                "query": "Research on machine learning",
                "entities": [{"text": "machine learning", "label": "CONCEPT"}],
                "relationships": [
                    {
                        "subject": "research",
                        "relation": "focus",
                        "object": "machine learning",
                    }
                ],
            },
        ]

        enhancement_times = []

        for test_case in test_cases:
            start_time = time.time()
            try:
                result = asyncio.run(
                    pipeline.enhance_query_with_relationships(
                        test_case["query"],
                        entities=test_case["entities"],
                        relationships=test_case["relationships"],
                    )
                )
                end_time = time.time()
                enhancement_time = end_time - start_time
                enhancement_times.append(enhancement_time)

                result.get("enhanced_query", test_case["query"])
                print(
                    f"Enhancement '{test_case['query'][:30]}...': {enhancement_time:.3f}s"
                )

            except Exception:
                end_time = time.time()
                enhancement_time = end_time - start_time
                enhancement_times.append(enhancement_time)
                print(f"Enhancement handled gracefully: {enhancement_time:.3f}s")

        if enhancement_times:
            avg_enhancement_time = sum(enhancement_times) / len(enhancement_times)
            print(f"Average enhancement time: {avg_enhancement_time:.3f}s")

            # Performance assertion
            assert (
                avg_enhancement_time < 10.0
            ), f"Average enhancement time too high: {avg_enhancement_time:.3f}s"

    def test_memory_usage_during_operations(self):
        """Monitor memory usage for memory leaks during sustained operations"""
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        print(f"Initial memory usage: {initial_memory:.1f} MB")

        # Pre-initialize components to get baseline after model loading
        routing_agent = EnhancedRoutingAgent()
        extractor = RelationshipExtractorTool()

        # Do one warmup operation to load models
        try:
            with patch("src.common.config.get_config") as mock_config:
                mock_config.return_value = {
                    "video_agent_url": "http://localhost:8002",
                    "summarizer_agent_url": "http://localhost:8003",
                    "detailed_report_agent_url": "http://localhost:8004",
                }

                asyncio.run(routing_agent.route_query("warmup query"))
            asyncio.run(extractor.extract_comprehensive_relationships("warmup query"))
        except Exception:
            pass

        # Get baseline after warmup (models loaded)
        baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"Baseline memory after model loading: {baseline_memory:.1f} MB")
        initial_growth = baseline_memory - initial_memory
        print(f"Memory used by model loading: {initial_growth:.1f} MB")

        memory_samples = []

        # Now test for memory leaks during actual operations
        for i in range(5):  # Reduced iterations to focus on leak detection
            try:
                query = f"Test query {i} for memory leak detection"

                with patch("src.common.config.get_config") as mock_config:
                    mock_config.return_value = {
                        "video_agent_url": "http://localhost:8002",
                        "summarizer_agent_url": "http://localhost:8003",
                        "detailed_report_agent_url": "http://localhost:8004",
                    }

                    asyncio.run(routing_agent.route_query(query))

                asyncio.run(extractor.extract_comprehensive_relationships(query))

                # Sample memory usage
                current_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_samples.append(current_memory)

            except Exception:
                # Continue monitoring even if operations fail
                current_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_samples.append(current_memory)

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        max_memory = max(memory_samples) if memory_samples else final_memory

        # Calculate memory growth from baseline (after model loading)
        operational_growth = final_memory - baseline_memory
        total_growth = final_memory - initial_memory

        print(f"Final memory usage: {final_memory:.1f} MB")
        print(f"Peak memory during operations: {max_memory:.1f} MB")
        print(f"Total memory growth: {total_growth:.1f} MB")
        print(
            f"Operational memory growth (excluding model loading): {operational_growth:.1f} MB"
        )

        # Memory leak detection - operational growth should be minimal
        assert (
            operational_growth < 1000
        ), f"Potential memory leak detected: {operational_growth:.1f} MB growth during operations"

        # Total memory usage should be reasonable but we allow for ML models
        assert (
            total_growth < 6000
        ), f"Total memory growth extremely high: {total_growth:.1f} MB - investigate"

        # Warn about memory usage patterns
        if operational_growth > 200:
            print(
                f"⚠️  Potential memory leak: {operational_growth:.1f} MB growth during operations"
            )
        if total_growth > 3000:
            print(
                f"🔥 High total memory usage: {total_growth:.1f} MB - ML models loaded"
            )

    def test_concurrent_request_capacity(self):
        """Test system capacity under concurrent load"""
        with patch("src.common.config.get_config") as mock_config:
            mock_config.return_value = {
                "video_agent_url": "http://localhost:8002",
                "summarizer_agent_url": "http://localhost:8003",
                "detailed_report_agent_url": "http://localhost:8004",
            }

            routing_agent = EnhancedRoutingAgent()

            # Generate concurrent queries
            num_concurrent = 5
            queries = [f"Concurrent test query {i}" for i in range(num_concurrent)]

            async def process_concurrent_load():
                tasks = []
                start_time = time.time()

                for query in queries:
                    task = routing_agent.route_query(query)
                    tasks.append(task)

                try:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    end_time = time.time()
                    return results, end_time - start_time
                except Exception as e:
                    end_time = time.time()
                    return [e] * len(queries), end_time - start_time

            try:
                results, total_time = asyncio.run(process_concurrent_load())

                success_count = sum(1 for r in results if not isinstance(r, Exception))
                throughput = len(queries) / total_time

                print(f"Concurrent capacity: {success_count}/{len(queries)} successful")
                print(f"Total time: {total_time:.3f}s")
                print(f"Throughput: {throughput:.1f} queries/second")

                # Performance assertions
                assert (
                    total_time < 30.0
                ), f"Concurrent processing took too long: {total_time:.3f}s"
                assert (
                    success_count >= len(queries) * 0.5
                ), f"Too many failures: {success_count}/{len(queries)}"

            except Exception as e:
                print(f"Concurrent load handled gracefully: {e}")
                assert True  # Graceful handling is acceptable


@pytest.mark.integration
class TestAdaptiveThresholdPerformance:
    """Performance tests for adaptive threshold learning"""

    def test_threshold_learning_performance(self):
        """Test performance of threshold learning under load"""
        learner = AdaptiveThresholdLearner()

        # Simulate performance samples
        num_samples = 100
        sample_times = []

        for i in range(num_samples):
            start_time = time.time()
            try:
                asyncio.run(
                    learner.record_performance_sample(
                        routing_success=i % 2 == 0,  # Alternate success/failure
                        routing_confidence=0.5 + (i % 50) / 100.0,  # Varying confidence
                        search_quality=0.6 + (i % 40) / 100.0,  # Varying quality
                        response_time=1.0 + (i % 30) / 10.0,  # Varying response time
                        user_satisfaction=0.7
                        + (i % 30) / 100.0,  # Varying satisfaction
                    )
                )
                end_time = time.time()
                sample_times.append(end_time - start_time)

            except Exception as e:
                end_time = time.time()
                sample_times.append(end_time - start_time)
                print(f"Sample {i} handled gracefully: {e}")

        if sample_times:
            avg_sample_time = sum(sample_times) / len(sample_times)
            max_sample_time = max(sample_times)

            print(
                f"Threshold learning performance - Avg: {avg_sample_time:.4f}s, Max: {max_sample_time:.4f}s"
            )
            print(f"Processed {num_samples} samples")

            # Performance assertions
            assert (
                avg_sample_time < 0.1
            ), f"Average sample processing too slow: {avg_sample_time:.4f}s"
            assert (
                max_sample_time < 1.0
            ), f"Max sample processing too slow: {max_sample_time:.4f}s"

            # Test threshold retrieval performance
            start_time = time.time()
            learner.get_threshold_value("routing_confidence")
            end_time = time.time()
            retrieval_time = end_time - start_time

            print(f"Threshold retrieval time: {retrieval_time:.4f}s")
            assert (
                retrieval_time < 0.01
            ), f"Threshold retrieval too slow: {retrieval_time:.4f}s"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
