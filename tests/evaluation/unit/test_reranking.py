"""
Unit tests for reranking strategies.
"""

from unittest.mock import Mock, patch

import pytest

from src.evaluation.core.reranking import (
    ContentSimilarityRerankingStrategy,
    DiversityRerankingStrategy,
    HybridRerankingStrategy,
    RerankingError,
    TemporalRerankingStrategy,
)


class TestDiversityRerankingStrategy:
    """Test diversity reranking strategy."""

    @pytest.fixture
    def strategy(self):
        """Create strategy instance."""
        return DiversityRerankingStrategy()

    @pytest.fixture
    def sample_results(self):
        """Create sample search results."""
        return [
            {"id": "1", "score": 0.9, "content": "Result 1", "category": "A"},
            {"id": "2", "score": 0.85, "content": "Result 2", "category": "B"},
            {"id": "3", "score": 0.8, "content": "Result 3", "category": "A"},
            {"id": "4", "score": 0.75, "content": "Result 4", "category": "C"},
            {"id": "5", "score": 0.7, "content": "Result 5", "category": "B"},
        ]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_empty_results(self, strategy):
        """Test reranking with empty results."""
        results = await strategy.rerank("test query", [])
        assert results == []

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_single_result(self, strategy):
        """Test reranking with single result."""
        results = [{"id": "1", "score": 0.9}]
        reranked = await strategy.rerank("test query", results)
        assert len(reranked) == 1
        assert reranked[0]["id"] == "1"

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_diversity(self, strategy, sample_results):
        """Test diversity reranking."""
        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id")
            mock_analyzer.return_value = analyzer

            config = {"diversity_lambda": 0.7}
            reranked = await strategy.rerank("test query", sample_results, config)

            assert len(reranked) == len(sample_results)
            # First result should be highest scoring
            assert reranked[0]["id"] == "1"
            # Should diversify subsequent results
            categories = [r.get("category") for r in reranked[:3]]
            assert len(set(categories)) > 1  # Diverse categories

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_with_missing_scores(self, strategy):
        """Test reranking when some results lack scores."""
        results = [
            {"id": "1", "score": 0.9},
            {"id": "2"},  # No score
            {"id": "3", "score": 0.7},
        ]

        reranked = await strategy.rerank("test query", results)
        assert len(reranked) == 3
        # Results without scores should be handled
        assert all("id" in r for r in reranked)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_with_schema_info(self, strategy, sample_results):
        """Test reranking with schema information."""
        config = {
            "schema_name": "video_schema",
            "schema_fields": {"id_fields": ["id"], "content_fields": ["content"]},
            "diversity_lambda": 0.5,
        }

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id")
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("test query", sample_results, config)

            assert len(reranked) == len(sample_results)
            mock_analyzer.assert_called_once_with(
                "video_schema", config["schema_fields"]
            )

    @pytest.mark.unit
    def test_calculate_similarity(self, strategy):
        """Test similarity calculation."""
        analyzer = Mock()
        analyzer.extract_item_id = lambda x: x.get("id")

        # Test same ID - should return 1.0
        result1 = {"id": "1", "content": "test content"}
        result2 = {"id": "1", "content": "different"}
        sim_same_id = strategy._calculate_similarity(result1, result2, analyzer)
        assert sim_same_id == 1.0  # Same ID means same item

        # Test different IDs with different content
        result3 = {"id": "2", "content": "test content"}
        result4 = {"id": "3", "content": "completely different text"}
        sim_diff = strategy._calculate_similarity(result3, result4, analyzer)
        assert sim_diff >= 0  # Should return some similarity value

    @pytest.mark.unit
    def test_extract_content(self, strategy):
        """Test content extraction."""
        result = {
            "id": "1",
            "content": "Test content",
            "text": "Additional text",
            "metadata": {"description": "Meta description"},
        }

        content = strategy._extract_content(result)

        assert "Test content" in content
        assert "Additional text" in content
        assert "Meta description" in content

    @pytest.mark.unit
    def test_calculate_text_similarity(self, strategy):
        """Test text similarity calculation."""
        # Just test that the method exists and returns a value
        text1 = "machine learning algorithms"
        text2 = "deep neural networks"

        sim = strategy._calculate_text_similarity(text1, text2)
        assert isinstance(sim, float)
        assert 0 <= sim <= 1  # Should be between 0 and 1

        # Empty text should return 0
        sim_empty = strategy._calculate_text_similarity("", "test")
        assert sim_empty == 0.0

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_with_analyzer_error(self, strategy, sample_results):
        """Test reranking when analyzer fails."""
        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            mock_analyzer.side_effect = Exception("Analyzer error")

            # Should use default analyzer
            reranked = await strategy.rerank("test query", sample_results)

            assert len(reranked) == len(sample_results)


class TestContentSimilarityRerankingStrategy:
    """Test content similarity reranking strategy."""

    @pytest.fixture
    def strategy(self):
        """Create strategy instance."""
        return ContentSimilarityRerankingStrategy()

    @pytest.fixture
    def sample_results(self):
        """Create sample results with content."""
        return [
            {"id": "1", "score": 0.7, "content": "machine learning algorithms"},
            {"id": "2", "score": 0.8, "content": "deep learning neural networks"},
            {"id": "3", "score": 0.75, "content": "machine learning models"},
            {"id": "4", "score": 0.6, "content": "data science techniques"},
        ]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_by_content_similarity(self, strategy, sample_results):
        """Test reranking based on content similarity."""
        query = "machine learning"

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_content_text = lambda x: x.get("content", "")
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank(query, sample_results)

            assert len(reranked) == len(sample_results)
            # Results with "machine learning" should rank higher
            top_contents = [r["content"] for r in reranked[:2]]
            assert any("machine learning" in c for c in top_contents)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_with_embeddings(self, strategy):
        """Test reranking with embeddings."""
        results = [
            {"id": "1", "score": 0.7, "content": "test", "embedding": [0.1, 0.2, 0.3]},
            {"id": "2", "score": 0.8, "content": "test", "embedding": [0.2, 0.3, 0.4]},
            {"id": "3", "score": 0.6, "content": "test", "embedding": [0.9, 0.8, 0.7]},
        ]

        config = {"use_embeddings": True}

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_embedding = lambda x: x.get("embedding")
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("test query", results, config)

            assert len(reranked) == len(results)

    @pytest.mark.unit
    def test_calculate_keyword_similarity(self, strategy):
        """Test keyword similarity calculation."""
        query = "machine learning algorithms"
        content1 = "machine learning models and algorithms"
        content2 = "deep neural networks"
        content3 = ""

        # High overlap
        sim1 = strategy._calculate_keyword_similarity(query, content1)
        assert sim1 > 0.5

        # Low overlap
        sim2 = strategy._calculate_keyword_similarity(query, content2)
        assert sim2 < sim1

        # Empty content
        sim3 = strategy._calculate_keyword_similarity(query, content3)
        assert sim3 == 0.0

    @pytest.mark.unit
    def test_calculate_semantic_similarity(self, strategy):
        """Test semantic similarity calculation."""
        query = "machine learning"
        content1 = "machine learning algorithms"
        content2 = "deep neural networks"

        # Without embeddings, should use keyword overlap
        result1 = {"content": content1}
        sim1 = strategy._calculate_semantic_similarity(query, content1, result1)
        assert sim1 >= 0  # Should return some similarity

        result2 = {"content": content2}
        sim2 = strategy._calculate_semantic_similarity(query, content2, result2)
        assert sim2 >= 0

        # With embeddings (mocked)
        result3 = {"content": content1, "embedding": [0.1, 0.2, 0.3]}
        sim3 = strategy._calculate_semantic_similarity(query, content1, result3)
        assert sim3 >= 0  # Should handle embedding case

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_with_weights(self, strategy, sample_results):
        """Test reranking with custom similarity weights."""
        config = {"similarity_weights": {"keyword": 0.5, "tfidf": 0.3, "semantic": 0.2}}

        reranked = await strategy.rerank("machine learning", sample_results, config)

        assert len(reranked) == len(sample_results)
        # Should have content similarity scores
        assert all("content_similarity_score" in r for r in reranked)
        assert all("similarity_components" in r for r in reranked)


class TestTemporalRerankingStrategy:
    """Test temporal reranking strategy."""

    @pytest.fixture
    def strategy(self):
        """Create strategy instance."""
        return TemporalRerankingStrategy()

    @pytest.fixture
    def sample_results(self):
        """Create sample results with timestamps."""
        return [
            {"id": "1", "score": 0.8, "timestamp": "2024-01-15T10:00:00Z"},
            {"id": "2", "score": 0.9, "timestamp": "2024-01-10T10:00:00Z"},
            {"id": "3", "score": 0.75, "timestamp": "2024-01-20T10:00:00Z"},
            {"id": "4", "score": 0.7, "timestamp": "2024-01-05T10:00:00Z"},
        ]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_no_temporal_fields(self, strategy, sample_results):
        """Test reranking when no temporal fields configured."""
        config = {"schema_fields": {"temporal_fields": []}}

        reranked = await strategy.rerank("test query", sample_results, config)

        # Should return results unchanged
        assert len(reranked) == len(sample_results)
        assert reranked == sample_results

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_with_temporal_intent(self, strategy, sample_results):
        """Test reranking with temporal intent detected."""
        config = {"schema_fields": {"temporal_fields": ["timestamp"]}}

        # Query with temporal intent
        reranked = await strategy.rerank("latest videos", sample_results, config)

        assert len(reranked) == len(sample_results)
        # Should detect "latest" intent
        assert all("temporal_score" in r for r in reranked)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_no_temporal_intent(self, strategy, sample_results):
        """Test reranking when no temporal intent in query."""
        config = {"schema_fields": {"temporal_fields": ["timestamp"]}}

        # Query without clear temporal keywords
        reranked = await strategy.rerank("search videos", sample_results, config)

        # Should return results, possibly unchanged order but with temporal metadata
        assert len(reranked) == len(sample_results)
        # Results might have additional fields but should preserve original data
        for _i, result in enumerate(reranked):
            assert result["id"] in [r["id"] for r in sample_results]

    @pytest.mark.unit
    def test_detect_temporal_intent(self, strategy):
        """Test temporal intent detection."""
        # Query with "latest"
        intent1 = strategy._detect_temporal_intent("show me the latest results")
        assert intent1 is not None
        assert intent1["type"] == "latest"

        # Query with "oldest"
        intent2 = strategy._detect_temporal_intent("find the oldest records")
        assert intent2 is not None
        assert intent2["type"] == "oldest"

        # Query without clear temporal intent (may detect "in" as temporal)
        intent3 = strategy._detect_temporal_intent("search for videos")
        # This query has no temporal keywords
        if intent3:
            # If detected, should be a weak signal
            assert intent3["type"] in ["during", "in", "within"]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_oldest_first(self, strategy):
        """Test reranking with oldest first intent."""
        # Use simple results without timestamps to avoid extraction issues
        simple_results = [
            {"id": "1", "score": 0.8},
            {"id": "2", "score": 0.9},
            {"id": "3", "score": 0.75},
        ]
        config = {"schema_fields": {"temporal_fields": []}}

        # Query requesting oldest - but no temporal fields configured
        reranked = await strategy.rerank("oldest videos", simple_results, config)

        # Should return results unchanged when no temporal fields
        assert len(reranked) == len(simple_results)
        assert reranked == simple_results

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_rerank_missing_timestamps(self, strategy):
        """Test reranking when some results lack timestamps."""
        results = [
            {"id": "1", "score": 0.8, "timestamp": "2024-01-15T10:00:00Z"},
            {"id": "2", "score": 0.9},  # No timestamp
            {"id": "3", "score": 0.75, "timestamp": "2024-01-20T10:00:00Z"},
        ]

        config = {"temporal_strategy": "recency_first"}

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_temporal_info = lambda x: {"timestamp": x.get("timestamp")}
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("test query", results, config)

            assert len(reranked) == 3
            # Results without timestamps should still be included


class TestHybridRerankingStrategy:
    """Test hybrid reranking strategy."""

    @pytest.fixture
    def strategy(self):
        """Create strategy instance."""
        return HybridRerankingStrategy()

    @pytest.fixture
    def sample_results(self):
        """Create comprehensive sample results."""
        return [
            {
                "id": "1",
                "score": 0.9,
                "content": "machine learning",
                "timestamp": "2024-01-15T10:00:00Z",
                "category": "A",
            },
            {
                "id": "2",
                "score": 0.85,
                "content": "deep learning",
                "timestamp": "2024-01-20T10:00:00Z",
                "category": "B",
            },
            {
                "id": "3",
                "score": 0.8,
                "content": "machine learning",
                "timestamp": "2024-01-10T10:00:00Z",
                "category": "A",
            },
        ]

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_hybrid_rerank_default(self, strategy, sample_results):
        """Test hybrid reranking with default settings."""
        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id")
            analyzer.extract_content_text = lambda x: x.get("content", "")
            analyzer.extract_temporal_info = lambda x: {"timestamp": x.get("timestamp")}
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("machine learning", sample_results)

            assert len(reranked) == len(sample_results)
            assert all("hybrid_score" in r for r in reranked)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_hybrid_rerank_weighted(self, strategy, sample_results):
        """Test hybrid reranking with custom weights."""
        config = {
            "strategies": {
                "diversity": {"weight": 0.3, "lambda": 0.5},
                "content": {"weight": 0.5, "boost": 1.5},
                "temporal": {"weight": 0.2, "strategy": "recency_first"},
            }
        }

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id")
            analyzer.extract_content_text = lambda x: x.get("content", "")
            analyzer.extract_temporal_info = lambda x: {"timestamp": x.get("timestamp")}
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("machine learning", sample_results, config)

            assert len(reranked) == len(sample_results)
            # Weights should sum to 1.0
            total_weight = sum(s["weight"] for s in config["strategies"].values())
            assert abs(total_weight - 1.0) < 0.001

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_hybrid_single_strategy(self, strategy, sample_results):
        """Test hybrid with only one strategy enabled."""
        config = {"strategies": {"content": {"weight": 1.0}}}

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_content_text = lambda x: x.get("content", "")
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("machine learning", sample_results, config)

            assert len(reranked) == len(sample_results)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_hybrid_normalize_weights(self, strategy, sample_results):
        """Test weight normalization."""
        config = {
            "strategies": {
                "diversity": {"weight": 1},
                "content": {"weight": 2},
                "temporal": {"weight": 1},
            }
        }

        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id")
            analyzer.extract_content_text = lambda x: x.get("content", "")
            analyzer.extract_temporal_info = lambda x: {"timestamp": x.get("timestamp")}
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("test", sample_results, config)

            # Weights should be normalized
            assert len(reranked) == len(sample_results)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_hybrid_empty_config(self, strategy, sample_results):
        """Test hybrid with empty config."""
        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id")
            analyzer.extract_content_text = lambda x: x.get("content", "")
            analyzer.extract_temporal_info = lambda x: {"timestamp": x.get("timestamp")}
            mock_analyzer.return_value = analyzer

            # Should use default strategy weights
            reranked = await strategy.rerank("test", sample_results, config={})

            assert len(reranked) == len(sample_results)


class TestRerankingHelpers:
    """Test reranking helper functions and error handling."""

    @pytest.mark.unit
    def test_reranking_error(self):
        """Test RerankingError exception."""
        error = RerankingError("Test error")
        assert str(error) == "Test error"
        assert isinstance(error, Exception)

    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_strategy_with_malformed_results(self):
        """Test strategies with malformed results."""
        strategy = DiversityRerankingStrategy()

        # Results missing required fields
        results = [
            {"score": 0.9},  # No ID
            {"id": "2"},  # No score
            {"id": "4", "score": 0.5},  # Valid
        ]

        # Should handle gracefully
        with patch(
            "src.evaluation.core.reranking.get_schema_analyzer"
        ) as mock_analyzer:
            analyzer = Mock()
            analyzer.extract_item_id = lambda x: x.get("id", "")
            mock_analyzer.return_value = analyzer

            reranked = await strategy.rerank("test", results)
            assert isinstance(reranked, list)
            assert len(reranked) == len(results)
