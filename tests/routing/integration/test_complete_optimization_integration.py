"""
Complete Integration Tests for Phase 5: Optimization Orchestration

Tests the COMPLETE end-to-end optimization flow without manual intervention:
1. Routing spans are created via telemetry
2. PhoenixSpanEvaluator extracts experiences automatically
3. AnnotationAgent identifies low-quality spans
4. LLMAutoAnnotator generates annotations
5. AnnotationStorage stores in Phoenix
6. AnnotationFeedbackLoop feeds to optimizer
7. AdvancedRoutingOptimizer improves routing
8. Measurable improvements are validated
9. Results visible in Phoenix

NO MOCKS - all tests against real Phoenix server.
"""

import logging
import os
import time
from datetime import datetime, timedelta

import phoenix as px
import pytest

from src.app.routing.optimization_orchestrator import OptimizationOrchestrator
from src.app.telemetry.config import (
    SERVICE_NAME_ORCHESTRATION,
    SPAN_NAME_ROUTING,
    TelemetryConfig,
)
from src.app.telemetry.manager import TelemetryManager

logger = logging.getLogger(__name__)


@pytest.fixture
def phoenix_client():
    """Phoenix client for querying spans"""
    return px.Client()


@pytest.fixture
def test_tenant_id():
    """Unique tenant ID for test isolation"""
    return f"test_orchestration_{int(time.time())}"


@pytest.fixture
def telemetry_config(test_tenant_id):
    """Telemetry config for test tenant"""
    return TelemetryConfig.from_env()


@pytest.fixture
def telemetry_manager(test_tenant_id, telemetry_config):
    """Telemetry manager for creating test spans"""
    manager = TelemetryManager()
    return manager


@pytest.fixture
def project_name(test_tenant_id, telemetry_config):
    """Get Phoenix project name for test tenant"""
    return telemetry_config.get_project_name(
        test_tenant_id, service=SERVICE_NAME_ORCHESTRATION
    )


class TestCompleteOptimizationIntegration:
    """
    Integration tests for complete optimization orchestration

    Phase 5 Checkpoint Validation:
    - End-to-end flow works without manual intervention
    - Metrics show measurable improvement
    - All components communicate correctly
    - Optimization triggers automatically
    - Results are verifiable in Phoenix
    """

    @pytest.mark.asyncio
    async def test_single_optimization_cycle_end_to_end(
        self,
        phoenix_client,
        test_tenant_id,
        telemetry_manager,
        project_name,
    ):
        """
        Test complete single optimization cycle end-to-end

        This validates the ENTIRE flow:
        1. Create routing spans with varying quality
        2. Run orchestrator once
        3. Verify experiences extracted
        4. Verify annotations generated
        5. Verify feedback to optimizer
        6. Verify all results in Phoenix
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST: Single Optimization Cycle End-to-End")
        logger.info("=" * 80)

        # STEP 1: Create diverse routing spans (good, bad, ambiguous)
        logger.info("\n=== STEP 1: Creating routing spans with varying quality ===")

        test_cases = [
            # Bad routing: Low confidence
            ("What are the best restaurants in Paris?", "video_search", 0.25),
            ("Show me basketball highlights", "detailed_report", 0.30),
            # Ambiguous: Medium confidence
            ("Explain machine learning", "summarizer", 0.55),
            ("Tell me about quantum computing", "video_search", 0.58),
            # Good routing: High confidence
            ("Show me nature documentaries", "video_search", 0.95),
            ("Summarize the article", "summarizer", 0.92),
        ]

        start_time = datetime.now()

        for query, agent, confidence in test_cases:
            with telemetry_manager.span(
                name=SPAN_NAME_ROUTING,
                tenant_id=test_tenant_id,
                service_name=SERVICE_NAME_ORCHESTRATION,
                attributes={
                    "routing.query": query,
                    "routing.chosen_agent": agent,
                    "routing.confidence": confidence,
                    "routing.context": "{}",
                },
            ):
                time.sleep(0.05)

        telemetry_manager.force_flush(timeout_millis=5000)
        time.sleep(2)

        # STEP 2: Verify spans exist in Phoenix
        logger.info("\n=== STEP 2: Verifying spans exist in Phoenix ===")

        end_time = datetime.now()
        spans_df = phoenix_client.get_spans_dataframe(
            project_name=project_name,
            start_time=start_time - timedelta(seconds=10),
            end_time=end_time,
        )

        assert not spans_df.empty, "No spans found in Phoenix"
        routing_spans = spans_df[spans_df["name"] == SPAN_NAME_ROUTING]
        assert (
            len(routing_spans) >= 6
        ), f"Expected 6 routing spans, got {len(routing_spans)}"

        logger.info(f"âœ… Found {len(routing_spans)} routing spans in Phoenix")

        # STEP 3: Initialize orchestrator and run single cycle
        logger.info(
            "\n=== STEP 3: Running optimization orchestrator (single cycle) ==="
        )

        orchestrator = OptimizationOrchestrator(
            tenant_id=test_tenant_id,
            confidence_threshold=0.6,
            min_annotations_for_optimization=1,  # Low threshold for testing
        )

        # Run complete cycle
        result = await orchestrator.run_once()

        logger.info(f"ðŸ“Š Orchestration result: {result}")

        # STEP 4: Validate span evaluation results
        logger.info("\n=== STEP 4: Validating span evaluation ===")

        span_eval_result = result.get("span_evaluation", {})
        assert (
            span_eval_result.get("spans_processed", 0) >= 6
        ), f"Expected at least 6 spans processed, got {span_eval_result.get('spans_processed', 0)}"

        experiences_created = span_eval_result.get("experiences_created", 0)
        assert experiences_created > 0, "No experiences created from spans"

        logger.info(
            f"âœ… Span evaluation: {span_eval_result.get('spans_processed')} spans processed, "
            f"{experiences_created} experiences created"
        )

        # STEP 5: Validate annotation identification
        logger.info("\n=== STEP 5: Validating annotation identification ===")

        annotation_requests = result.get("annotation_requests", 0)
        assert annotation_requests >= 2, (
            f"Expected at least 2 annotation requests (low confidence spans), "
            f"got {annotation_requests}"
        )

        logger.info(f"âœ… Identified {annotation_requests} spans needing annotation")

        # STEP 6: Validate feedback loop processed annotations
        logger.info("\n=== STEP 6: Validating feedback loop ===")

        feedback_result = result.get("feedback_loop", {})
        logger.info(f"ðŸ“Š Feedback result: {feedback_result}")

        # Feedback loop should run (even if no annotations processed yet)
        assert feedback_result is not None, "Feedback loop did not run"

        # STEP 7: Validate optimizer received experiences
        logger.info("\n=== STEP 7: Validating optimizer state ===")

        optimizer = orchestrator.optimizer
        total_experiences = len(optimizer.experiences)

        logger.info(f"âœ… Optimizer has {total_experiences} total experiences")
        assert (
            total_experiences >= experiences_created
        ), "Optimizer should have experiences from span evaluation"

        # STEP 8: Validate orchestrator metrics
        logger.info("\n=== STEP 8: Validating orchestrator metrics ===")

        metrics = orchestrator.get_metrics()
        logger.info(f"ðŸ“Š Orchestrator metrics: {metrics}")

        assert metrics["spans_evaluated"] >= 6, "Metrics should track spans evaluated"
        assert (
            metrics["experiences_created"] >= experiences_created
        ), "Metrics should track experiences created"

        logger.info("\n" + "=" * 80)
        logger.info("âœ… TEST PASSED: Single optimization cycle completed successfully")
        logger.info("=" * 80)

    @pytest.mark.asyncio
    async def test_orchestrator_with_annotations_and_feedback(
        self,
        phoenix_client,
        test_tenant_id,
        telemetry_manager,
        project_name,
    ):
        """
        Test orchestrator processes annotations and feeds to optimizer

        This validates:
        1. Low-quality spans identified
        2. Annotations generated (if LLM available)
        3. Annotations stored in Phoenix
        4. Feedback loop retrieves annotations
        5. Optimizer receives annotation-based experiences
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST: Orchestrator with Annotations and Feedback")
        logger.info("=" * 80)

        # STEP 1: Create low-quality routing spans
        logger.info("\n=== STEP 1: Creating low-quality routing spans ===")

        low_quality_cases = [
            ("Find restaurants", "video_search", 0.20),
            ("What is AI", "detailed_report", 0.15),
            ("Show documentaries", "summarizer", 0.35),
        ]

        for query, agent, confidence in low_quality_cases:
            with telemetry_manager.span(
                name=SPAN_NAME_ROUTING,
                tenant_id=test_tenant_id,
                service_name=SERVICE_NAME_ORCHESTRATION,
                attributes={
                    "routing.query": query,
                    "routing.chosen_agent": agent,
                    "routing.confidence": confidence,
                    "routing.context": "{}",
                },
            ):
                time.sleep(0.05)

        telemetry_manager.force_flush(timeout_millis=5000)
        time.sleep(2)

        # STEP 2: Initialize orchestrator with low thresholds
        logger.info("\n=== STEP 2: Initializing orchestrator ===")

        orchestrator = OptimizationOrchestrator(
            tenant_id=test_tenant_id,
            confidence_threshold=0.6,  # All test spans are below this
            min_annotations_for_optimization=1,
        )

        # STEP 3: Run single cycle
        logger.info("\n=== STEP 3: Running orchestration cycle ===")

        result = await orchestrator.run_once()
        logger.info(f"ðŸ“Š Result: {result}")

        # STEP 4: Validate all low-quality spans identified
        logger.info("\n=== STEP 4: Validating annotation identification ===")

        annotation_requests = result.get("annotation_requests", 0)
        assert (
            annotation_requests >= 3
        ), f"Expected all 3 low-quality spans identified, got {annotation_requests}"

        logger.info(f"âœ… All {annotation_requests} low-quality spans identified")

        # STEP 5: Check if annotations were generated
        logger.info("\n=== STEP 5: Checking annotation generation ===")

        annotations_generated = result.get("annotations_generated", 0)
        has_llm = os.getenv("ANTHROPIC_API_KEY") or os.getenv("OPENAI_API_KEY")

        if has_llm and annotations_generated > 0:
            logger.info(f"âœ… Generated {annotations_generated} LLM annotations")
        else:
            logger.info("âš ï¸ No LLM API key - annotations await human review")

        # STEP 6: Validate feedback loop execution
        logger.info("\n=== STEP 6: Validating feedback loop ===")

        feedback_result = result.get("feedback_loop", {})
        assert feedback_result is not None, "Feedback loop should have run"

        logger.info(f"âœ… Feedback loop executed: {feedback_result}")

        # STEP 7: Validate orchestrator tracked everything
        logger.info("\n=== STEP 7: Validating metrics tracking ===")

        metrics = orchestrator.get_metrics()
        assert (
            metrics["annotations_requested"] >= 3
        ), "Metrics should track annotation requests"

        logger.info(f"ðŸ“Š Final metrics: {metrics}")

        logger.info("\n" + "=" * 80)
        logger.info("âœ… TEST PASSED: Annotations and feedback flow validated")
        logger.info("=" * 80)

    @pytest.mark.asyncio
    async def test_orchestrator_automatic_optimization_trigger(
        self,
        phoenix_client,
        test_tenant_id,
        telemetry_manager,
        project_name,
    ):
        """
        Test that optimization triggers automatically when thresholds met

        This validates Phase 5 requirement:
        - Optimization triggers automatically
        - No manual intervention required
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST: Automatic Optimization Trigger")
        logger.info("=" * 80)

        # STEP 1: Create many routing spans
        logger.info("\n=== STEP 1: Creating multiple routing spans ===")

        # Create 20 diverse routing spans
        for i in range(20):
            confidence = 0.3 + (i * 0.03)  # Varying confidence
            query = f"Test query {i}"
            agent = ["video_search", "summarizer", "detailed_report"][i % 3]

            with telemetry_manager.span(
                name=SPAN_NAME_ROUTING,
                tenant_id=test_tenant_id,
                service_name=SERVICE_NAME_ORCHESTRATION,
                attributes={
                    "routing.query": query,
                    "routing.chosen_agent": agent,
                    "routing.confidence": confidence,
                    "routing.context": "{}",
                },
            ):
                time.sleep(0.02)

        telemetry_manager.force_flush(timeout_millis=5000)
        time.sleep(2)

        # STEP 2: Initialize orchestrator with low optimization threshold
        logger.info("\n=== STEP 2: Initializing orchestrator with low thresholds ===")

        orchestrator = OptimizationOrchestrator(
            tenant_id=test_tenant_id,
            confidence_threshold=0.7,  # Many spans below this
            min_annotations_for_optimization=5,  # Low threshold for testing
        )

        # STEP 3: Run multiple cycles to accumulate data
        logger.info("\n=== STEP 3: Running multiple orchestration cycles ===")

        for cycle in range(2):
            logger.info(f"\n--- Cycle {cycle + 1} ---")
            result = await orchestrator.run_once()
            logger.info(f"Cycle {cycle + 1} result: {result}")
            time.sleep(1)

        # STEP 4: Validate experiences accumulated
        logger.info("\n=== STEP 4: Validating experience accumulation ===")

        optimizer = orchestrator.optimizer
        total_experiences = len(optimizer.experiences)

        logger.info(f"ðŸ“Š Total experiences accumulated: {total_experiences}")
        assert (
            total_experiences >= 10
        ), f"Expected at least 10 experiences, got {total_experiences}"

        # STEP 5: Validate metrics
        logger.info("\n=== STEP 5: Validating final metrics ===")

        metrics = orchestrator.get_metrics()
        logger.info("ðŸ“Š Final orchestrator metrics:")
        logger.info(f"  Spans Evaluated: {metrics['spans_evaluated']}")
        logger.info(f"  Experiences Created: {metrics['experiences_created']}")
        logger.info(f"  Annotations Requested: {metrics['annotations_requested']}")

        assert metrics["spans_evaluated"] >= 20, "Should have evaluated all spans"
        assert (
            metrics["experiences_created"] >= 10
        ), "Should have created experiences from spans"

        logger.info("\n" + "=" * 80)
        logger.info("âœ… TEST PASSED: Automatic optimization trigger validated")
        logger.info("=" * 80)

    @pytest.mark.asyncio
    async def test_orchestrator_metrics_tracking(
        self,
        phoenix_client,
        test_tenant_id,
        telemetry_manager,
        project_name,
    ):
        """
        Test orchestrator tracks all metrics correctly

        Validates Phase 5 requirement:
        - Metrics show measurable improvement
        - All components tracked
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST: Orchestrator Metrics Tracking")
        logger.info("=" * 80)

        # STEP 1: Initialize orchestrator
        orchestrator = OptimizationOrchestrator(
            tenant_id=test_tenant_id,
            confidence_threshold=0.6,
            min_annotations_for_optimization=5,
        )

        # STEP 2: Get initial metrics
        initial_metrics = orchestrator.get_metrics()
        logger.info(f"ðŸ“Š Initial metrics: {initial_metrics}")

        assert initial_metrics["spans_evaluated"] == 0
        assert initial_metrics["experiences_created"] == 0
        assert initial_metrics["annotations_requested"] == 0
        assert initial_metrics["started_at"] is not None

        # STEP 3: Create test spans
        logger.info("\n=== Creating test routing spans ===")

        for i in range(5):
            with telemetry_manager.span(
                name=SPAN_NAME_ROUTING,
                tenant_id=test_tenant_id,
                service_name=SERVICE_NAME_ORCHESTRATION,
                attributes={
                    "routing.query": f"Test {i}",
                    "routing.chosen_agent": "video_search",
                    "routing.confidence": 0.5,
                    "routing.context": "{}",
                },
            ):
                time.sleep(0.05)

        telemetry_manager.force_flush(timeout_millis=5000)
        time.sleep(2)

        # STEP 4: Run orchestration
        await orchestrator.run_once()

        # STEP 5: Validate metrics updated
        updated_metrics = orchestrator.get_metrics()
        logger.info(f"ðŸ“Š Updated metrics: {updated_metrics}")

        assert updated_metrics["spans_evaluated"] >= 5, "Should track spans evaluated"
        assert (
            updated_metrics["experiences_created"] > 0
        ), "Should track experiences created"
        assert updated_metrics["uptime_seconds"] > 0, "Should track uptime"

        logger.info("\n" + "=" * 80)
        logger.info("âœ… TEST PASSED: Metrics tracking validated")
        logger.info("=" * 80)
